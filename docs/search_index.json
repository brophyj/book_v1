[["index.html", "(Mostly Clinical) Epidemiology with R About the Author", " (Mostly Clinical) Epidemiology with R James Brophy 2021-03-02 About the Author James (Jay) Brophy is a full professor with a joint appointment in the Departments of Medicine and Epidemiology and Biostatistics at McGill University where he works as a clinical cardiologist at the McGill University Health Center and does research in cardiovascular epidemiology. His research interests are eclectic and include clinical and outcomes research, pharmacoepidemiology, health technology assessment, and economic analyses. "],["simple-acknowledgment-and-request.html", "Simple acknowledgment and request", " Simple acknowledgment and request In writing this book, I was amazed at how much of this material I either never learned at all or didn’t assimilate completely on my first, and occasionally second and third, efforts. Consequently, I’m sure there are imprecisions, clarifications and outright errors that will need addressing. For reason that I outline in Section 1.5, I am releasing the first six chapters as “volume 1.” Hopefully, other chapters will be forthcoming in a timely manner. I would welcome all comments that may bring undoubted much needed clarifications, corrections, and improvements, including optimization of my R code. These may be addressed to me either by email or via Github. I should mention that any messages that begin “You ass@@@@ ….” will probably be ignored. "],["intro.html", "Chapter 1 Introduction 1.1 Preface 1.2 Prerequisites 1.3 My general philosophy for clinical epidemiology 1.4 Brief excursions into quantification 1.5 Outline 1.6 Statistical software - R 1.7 R - Common data and variable manipulations 1.8 RStudio - The IDE for R 1.9 R - More than a statistical program 1.10 R - General Public License", " Chapter 1 Introduction 1.0.1 R packages required for this chapter library(knitr) library(tidyverse) 1.1 Preface This is an intermediate epidemiology book that focuses on the methods most commonly encountered in problems arising in the domain of clinical epidemiology. The emphasis is on research design, quantitative analysis, critical thinking, and reproducible research methods. As it is my belief that epidemiologic principles are fundamental to high quality medical research and the learning and evaluation of these principles are best consolidated through direct application to problem solving, the text emphasizes the use of quantification, via open source software. By the end of the book students can expect to: • Understand and critically evaluate basic study designs and statistical inferences in medical research • Understand, identify, and quantitatively evaluate sources of bias in epidemiologic and clinical studies • Master quantitative techniques for visualizing, cleaning, analyzing and interpreting epidemiologic and clinical data, including descriptive analyses, stratified analyses, regression analyses and meta-analyses. 1.2 Prerequisites There are multiple excellent introductory (Gordis 2014) (Rothman 2012), intermediate (Szklo and Nieto 2019) and advanced (Rothman, Greenland, and Lash 2008) epidemiology textbooks. There are no obligatory prerequisites for this book, although its use will be easier if there has been some prior exposure to basic epidemiologic concepts. 1.3 My general philosophy for clinical epidemiology The standard definition of epidemiology is the study of the distribution and determinants of health-related states or events in specified populations, and the application of this study to the control of health problems (Last 2001). This definition may be expanded as follows: * The distribution of health-related states refers to analysis by time, place, and population. * The different varieties of epidemiologic study include; i) Descriptive: looking at patterns of disease within and between populations, trying to understand why disease rates go up and down, population variation ii) Etiologic: understand the causes of disease, with implications for interventions to change disease distributions iii) Predictive: predict outcomes for future patients, with implications for interventions to alter outcomes * The common determinants of of health-related states are biological, behavioral, social, cultural, economic, and political factors that influence health * The ultimate goal of epidemiology is the advancement of scientific knowledge to promote, protect, and restore health Clinical epidemiology may therefore be seen (?defined) as the application of epidemiologic principles and reasoning to improve clinical decision making and ultimately “patient” outcomes. However, I think it is preferable to interpret the term “patient” in the widest possible context, and consequently clinical epidemiology might range from the study of preventive measures in asymptomatic individuals to the evaluation of quaternary hospital interventions in large cohorts. As patient cohorts may arise not only from hospitals but also from specific neighborhoods, cities, provinces and countries the distinction between classic epidemiology and clinical epidemiology is hardly distinct. Consequently, while this book will focus on clinical applications, the underlying methods of classic epidemiology will also be presented, admittedly in a focused or condensed manner. Epidemiology is the quantitative heart of both public health &amp; clinical medicine (i.e. the science of counting within and across populations). This book will emphasize a hands-on quantitative approach to clinical epidemiology. Realizations that “if you can’t measure something, you can’t improve it” and “intuition without quantification is often too imprecise to be useful” support the importance of a rigorous quantitative approach. Epidemiology concepts may be vague, or plain hard to understand, and practical data manipulation provides opportunities to reinforce our understanding of these methods making us both better consumers and producers of medical research. While this book stresses a quantitative approach, one must be careful to avoid the McNamara or the quantifiable fallacy “if you can’t measure what is important, make what you can measure important,” first documented during the Vietnam war, but similar erroneous uses of quantification may arise in clinical medicine. For example, progression-free survival (PFS) as a primary endpoint in many cancer trials largely because it is an endpoint which is easily measurable, but it meaningfulness compared to other metrics such as overall quality of life or overall survival is highly questionable.(Kim and Prasad 2015) Extensive quantification of biased results are also of no value. Consequently this book will also concentrate on recognizing common biases and assuring the application of methodologic rigor at all levels of study design, analysis, interpretation, and dissemination. 1.4 Brief excursions into quantification This is a brief excursions into quantification or why intuition is not good enough. Herein I offer several examples that hopefully highlight the need for careful quantification in order to reach reasonable conclusions. 1. Birthday problem First, consider the birthday problem, a common parlor game, which shows how we may underestimate the play of chance. Imagine you are in a class of 60 people, what is the probability that at least 2 people share the same birthday? Potential answers ## A. &lt; 5% ## B. 5 - 15% ## C. 15 - 25% ## D. &gt;99% Most people, who aren’t already aware of the “trick” or “paradox” assume the probability is small given that a birthday may in occur on any one of 365 possible days. Given our natural tendency to egocentricity they imagine there are 59 chances of some else having the same birthday as them, or a probability of 59/365 or 16.2%. However this ignores the other 1711 (5958) possible combinations between someone who is not you being compared to someone else who is also not you. Also as we will see, calculations involving exponentials are hardly intuitive to most people who tend to think in linearities. As an easy solution, begin by considering the reverse problem what is the probability that no one has the same birthday as you. For the 2nd person, first next to you, probability of not being born on the same day as you = 364/365. So the probability that 3rd person not on the same day as first 2 persons = 364/365 363/365 and The probability i th person not on the same day as 1st i-1 = 364/365 * 363/365 * 362/365 * (n-1)/365 or \\[ \\prod_{i=0}^{i-1} (1- \\dfrac{i}{365}) \\] Then the probability of being born on the same day = 1 - probability not being born on the same day. These calculations can be easily performed in R. # create a birthday function to calculate probability of 2 people in a group (with a default group of 23) # default of 23 since that gives probbaility = 50% birthday &lt;- function(n=23){ 1-prod(1-(0:(n-1))/365) } birthday(60) ## [1] 0.9941 # see why the default is 23 -&gt; 50% probability birthday() ## [1] 0.5073 p &lt;- numeric(100) for(i in 1:100){ p[i] &lt;- birthday(i) } p &lt;- data.frame(x=seq(1,100, 1), p=p) ggplot(p, aes(x,p)) + geom_line() + scale_y_continuous(labels = scales::percent_format()) + labs(y = &quot;Probability&quot;) + ggtitle(&quot;Probability of same birthday as a function of sample size&quot;) + geom_segment(aes(x = 0, y = .50, xend = 23, yend = .50), color=&quot;red&quot;) + geom_segment(aes(x = 23, y = .50, xend = 23, yend = 0), color=&quot;red&quot;) + theme_bw() So the probability of finding 2 people in a class of 60 with the same birthday is &gt;99%. The graph also shows that provided there are more than 23 people in the class the probability will be &gt; 50%. One can alaso appreciate the non-linearity of the relationship. To give an idea about the range of functions in R it should be noted that you don’t have to write your own birthday function as above but can simple call the built in pbirthday function. One can also verify these calculations by doing simulations as demonstrated in this blog. # using the built in R function cat(&quot;The probability of 2 people with the same bithday in a group of 23 is &quot;, pbirthday(23), &quot; and for a group of 60 is &quot;, pbirthday(60)) ## The probability of 2 people with the same bithday in a group of 23 is 0.5073 and for a group of 60 is 0.9941 2. Voter suppression Now being able to solve party games is a good, but not the only, reason to develop your quantitative skills. Consider the very current issue of voter suppression. A news story reported that in Georgia, where 32% of the population is black, the state has recently removed 670,000 voters of whom 70% were black. Is this difference likely due to the play of chance? In other words, how likely are we to observe 469,000 (.7 * 670,000) blacks out of 670,000 if the selection probability is the same as the proportion of blacks in the general population (32%). This can be found with 1 line of code. Additional information about this function can be found with help(binom.test). binom.test(469000, 670000, .32) ## ## Exact binomial test ## ## data: 469000 and 670000 ## number of successes = 469000, number of trials = 670000, p-value &lt;2e-16 ## alternative hypothesis: true probability of success is not equal to 0.32 ## 95 percent confidence interval: ## 0.6989 0.7011 ## sample estimates: ## probability of success ## 0.7 It appears highly unlikely to have excluded htis number of black voters if the null hypothesis is that voter suppression is independent of race. 3. Avoiding disasters In 2018, medical researchers form John Hopkins reported a clinical trial in JAMA entitled “Effect of a Program Combining Transitional Care and Long-term Self-management Support on Outcomes of Hospitalized Patients With Chronic Obstructive Pulmonary Disease A Randomized Clinical Trial” that concluded the intervention lead to “significantly fewer COPD related hospitalizations.” The next year the authors published a retraction and a new article that concluded the intervention lead to “significantly greater COPD related hospitalizations.” The problem was a simple miscoding of the intervention and control arms. Being able to read and follow code, can assure that extra eyes are available to catch these errors which can happen to all of us. While reviewing their code to correct this major error, the research team discovered at least two other areas of erroneous code (in the commands to impute missing values and to aggregate data into summary variables). On a broader level, sharing analytic code is increasingly the norm across many fields and provides an unambiguous record of the analytical methods used, aiding reproducibility, helping avoid duplicated effort and possibly accelerating innovation. it would be nice to see this become the norm in clinical epidemiology. 1.5 Outline The scientific discipline of epidemiology has greatly expanded from its origin narrow focus on communicable disease epidemics to broadly include all phenomena related to population health. The aim of this book is assist in developing the design and analytical skills to critically evaluate (and produce) medical research that avoids biases and quantitative errors. The topics to be discussed will hopefully assist healthcare personnel in reaching the goal of being better consumers and producers of an ever expanding medical literature. For a variety of reasons, not the least of which is my inexperience and lack of knowledge about publishing a html book, I have decided to make this book into two or three volumes. This will also allow me to release material more quickly and hopefully get feedback to correct the errors, incorporate suggestions, and generally improve the overall content. Table of contents (volume 1) Chapter Topic Chapter 1 Introduction to clinical epidemiology Chapter ?? Introduction to statistical software - R Chapter 2 Exploratory Data Analysis - Data visualization Chapter 3 Contingency tables, measures of association &amp; R packages Chapter 4 Statistical inference Chapter 5 Non-experimental designs &amp; incidence measures Chapter 6 References Future volumes are likely to include the following topics Causal inference Confounding Effect Measure Modification &amp; interaction Stratification &amp; adjustment Overview of biases (selection and information) Quantitative bias analysis Poisson regression Survival analysis Propensity Scores / Instrumental variables Randomized clinical trials Meta-analysis Pharmacoepidemiology Evidence based medicine / Guidelines Health economics / ethics &lt;!--chapter:end:01-intro.Rmd--&gt; # Introduction to statistical software - R {#soft} ### R packages required for this chapter ```r library(knitr) library(tidyverse) library(broom) library(psych) library(magrittr) A twitter post has nicely summarized 10 reasons “Why should I bother learning to code”; Encourages reproducible statistical analyses Enables easy incorporation of “New” and “personalized” statistical methods Code sharing - be inspired and inspiring Career perspectives increased with this skill Data visualization -&gt; better understanding of your data Avoid copy / paste frustrations Customization - get exactly what you want Develop interactive graphs and web-apps to increase dissemination and understanding of your work Consults with a statistician easier Personal satisfaction (it can be fun) In summary, coding has the attributes of flexibility, transparency, and reproduciblity which should enhance overall research quality. 1.6 Statistical software - R The most important element in clinical epidemiology is NOT which statistical software is chosen but rather an in depth understanding the basic epidemiologic and statistical concepts. Having said that, there are many advantages for R, largely summarized by the fact that R is the lingua franca of data science, used by millions of data experts. Why R? Free and open source software environment for statistical computing and graphics Open source indicates the original source code is freely available, may be redistributed, and modified Allows &amp; encourages researchers to modify, extend, and develop additions to the base program Additions are referred to as packages Use of scripts and Rmarkdown encourages reproducible research Active online community facilitates formal courses, sharing of solutions to coding queries Rstudio, an integrated development environment (IDE) greatly facilitates the R experience Combining with Rmarkdown can easily create, reproduce and share your work via html or pdf files This book is not intended to be first line resource for learning R, as there are many excellent online learning resources. It should be noted that there are at least 2 flavors or R - 1) standard base R 2) tidyverse version, a collection of R packages designed with a common philosophy, grammar, and data structures especially useful for data science. Learning and help resources R definitive online resource can be found at CRAN has a number of manuals online Condensed R reference card can be found [here][https://cran.r-project.org/doc/contrib/Short-refcard.pdf] The swirl tutorial teaches R programming and data science interactively, install swirl with install.packages(\"swirl\") and run with the swirl() command Helpful cheet sheets can be found as the RStudio website UCLA Quick R R blogger, a daily compilation of R blogs from over the interent Advanced R After acquiring the basics, many questions are answered with the help of Stackoverflow Good old Google using “r type your question” Within the R environment to find help for a specific function, for example epi.2by2 in the EpiR package try typing * help(\"epi.2by2\") * example(\"epi.2by2\") * help.search(\"epi.2by2\") * RSiteSearch(“epi.2by2”) - provides online search Packages The capabilities of base R are greatly extended using “packages.” These are distributed over the Internet via CRAN and can be downloaded either directly during an R session by typing the command install.packages(\"pakage.name\"). Alternatively this can be done via RStudio which also provides a directory of all downloaded and installed packages. In 2010, there were about 2,000 packages, by 2016 there were almost 10,000 and by 2020 this has reached almost 17,000. This rapid growth of these important resources is one of the prime reasons for the ever increasing popularity of R. Of course, there is also a chick and egg argument that sees the increasing popularity of R as a reason why more people are contributing packages. For epidemiologists some of the standard epidemiology packages include epiR, epibasix, epitools, and Epi but there are over 30 packages including some that are ultra specialized. Figure 1.1: Epi packages available on CRAN 1.7 R - Common data and variable manipulations R is a programming language based on the concept of objects, which may be data or code, in the form of procedures. The data structures are a form of organizing and storing data are four basic types - vector (single dimension structure of 1 type), matrix (two dimension structure of 1 type), list (single dimensional data structure of different types), &amp; data frame (special case of a list where each component is of same length). Data frames are the most common data structure used in epidemiology analyses. Here are some common data manipulations in R that represent the minimal knowledge or comfortable level that the reader may like to have to easily follow the code in later chapters. Creating a data frame # creation of a simple data frame (dat) dat &lt;- data.frame(&#39;id&#39;=1:4, &#39;Age&#39;=c(21,15,14,18), &#39;Gender&#39;=c(&#39;M&#39;,&#39;F&#39;,&#39;F&#39;,&#39;M&#39;)) dat ## id Age Gender ## 1 1 21 M ## 2 2 15 F ## 3 3 14 F ## 4 4 18 M Read a data file dat1 &lt;- read.csv(&quot;data/pima_db.csv&quot;) head(dat1,3) ## Pregnancies Glucose BloodPressure SkinThickness Insulin BMI ## 1 6 148 72 35 0 33.6 ## 2 1 85 66 29 0 26.6 ## 3 8 183 64 0 0 23.3 ## DiabetesPedigreeFunction Age Outcome ## 1 0.627 50 1 ## 2 0.351 31 0 ## 3 0.672 32 1 Other file formats including Excel, SAS, Stata, SPSS files can be read with readxl::read_excel(), sas7bdat::read.sas7bdat(), Hmisc::spss.get(), foreign::read.dta() respectively. Variable manipulation # create a new variable based on cutoff on existing variable # Base R dat1$Glucose_hi &lt;- NA dat1[dat1$Glucose &gt;120, &#39;Glucose_hi&#39;] &lt;- 1 dat1[dat1$Glucose &lt;=120, &#39;Glucose_hi&#39;] &lt;- 0 head(dat1[,c(1:3,8:10)],4) ## Pregnancies Glucose BloodPressure Age Outcome Glucose_hi ## 1 6 148 72 50 1 1 ## 2 1 85 66 31 0 0 ## 3 8 183 64 32 1 1 ## 4 1 89 66 21 0 0 #tidyverse library(tidyverse) dat2 &lt;- dat1 %&gt;% mutate(Age_old = ifelse(Age &gt; 50, 1, 0)) head(dat2[,c(1:3,8:11)],10) ## Pregnancies Glucose BloodPressure Age Outcome Glucose_hi Age_old ## 1 6 148 72 50 1 1 0 ## 2 1 85 66 31 0 0 0 ## 3 8 183 64 32 1 1 0 ## 4 1 89 66 21 0 0 0 ## 5 0 137 40 33 1 1 0 ## 6 5 116 74 30 0 0 0 ## 7 3 78 50 26 1 0 0 ## 8 10 115 0 29 0 0 0 ## 9 2 197 70 53 1 1 1 ## 10 8 125 96 54 1 1 1 Variable and data subsetting ################# # Variable subsetting ################# # Base R dat1s = subset(dat1, select = c(&#39;Pregnancies&#39;, &#39;Glucose&#39;)) head(dat1s) ## Pregnancies Glucose ## 1 6 148 ## 2 1 85 ## 3 8 183 ## 4 1 89 ## 5 0 137 ## 6 5 116 # tidyverse dat1 %&gt;% dplyr::select(Pregnancies, Glucose) %&gt;% head() ## Pregnancies Glucose ## 1 6 148 ## 2 1 85 ## 3 8 183 ## 4 1 89 ## 5 0 137 ## 6 5 116 ################# # Data subsetting ################# # Base R #1 dat1s &lt;- subset(dat1, subset = Pregnancies &gt;2 &amp; Glucose_hi == 1) # notice need for == when looking for equality head(dat1s[,c(1:4,8:10)]) ## Pregnancies Glucose BloodPressure SkinThickness Age Outcome Glucose_hi ## 1 6 148 72 35 50 1 1 ## 3 8 183 64 0 32 1 1 ## 10 8 125 96 0 54 1 1 ## 12 10 168 74 0 34 1 1 ## 13 10 139 80 0 57 0 1 ## 15 5 166 72 19 51 1 1 # Base R #2 dat1ss = dat1[which(dat1$Pregnancies &gt;2 &amp; dat1$Glucose_hi ==1),] head(dat1ss[,c(1:4,8:10)]) ## Pregnancies Glucose BloodPressure SkinThickness Age Outcome Glucose_hi ## 1 6 148 72 35 50 1 1 ## 3 8 183 64 0 32 1 1 ## 10 8 125 96 0 54 1 1 ## 12 10 168 74 0 34 1 1 ## 13 10 139 80 0 57 0 1 ## 15 5 166 72 19 51 1 1 # tidyverse library(tidyverse) dat1 %&gt;% dplyr::filter(Pregnancies &gt;2 &amp; Glucose_hi == 1) %&gt;% head(,c(1:4,8:10)) ## Pregnancies Glucose BloodPressure SkinThickness Insulin BMI ## 1 6 148 72 35 0 33.6 ## 2 8 183 64 0 0 23.3 ## 3 8 125 96 0 0 0.0 ## 4 10 168 74 0 0 38.0 ## 5 10 139 80 0 0 27.1 ## 6 5 166 72 19 175 25.8 ## DiabetesPedigreeFunction Age Outcome Glucose_hi ## 1 0.627 50 1 1 ## 2 0.672 32 1 1 ## 3 0.232 54 1 1 ## 4 0.537 34 1 1 ## 5 1.441 57 0 1 ## 6 0.587 51 1 1 Basic Data Descriptions # Base R summary(dat1) ## Pregnancies Glucose BloodPressure SkinThickness Insulin ## Min. : 0.00 Min. : 0 Min. : 0.0 Min. : 0.0 Min. : 0.0 ## 1st Qu.: 1.00 1st Qu.: 99 1st Qu.: 62.0 1st Qu.: 0.0 1st Qu.: 0.0 ## Median : 3.00 Median :117 Median : 72.0 Median :23.0 Median : 30.5 ## Mean : 3.85 Mean :121 Mean : 69.1 Mean :20.5 Mean : 79.8 ## 3rd Qu.: 6.00 3rd Qu.:140 3rd Qu.: 80.0 3rd Qu.:32.0 3rd Qu.:127.2 ## Max. :17.00 Max. :199 Max. :122.0 Max. :99.0 Max. :846.0 ## BMI DiabetesPedigreeFunction Age Outcome ## Min. : 0.0 Min. :0.078 Min. :21.0 Min. :0.000 ## 1st Qu.:27.3 1st Qu.:0.244 1st Qu.:24.0 1st Qu.:0.000 ## Median :32.0 Median :0.372 Median :29.0 Median :0.000 ## Mean :32.0 Mean :0.472 Mean :33.2 Mean :0.349 ## 3rd Qu.:36.6 3rd Qu.:0.626 3rd Qu.:41.0 3rd Qu.:1.000 ## Max. :67.1 Max. :2.420 Max. :81.0 Max. :1.000 ## Glucose_hi ## Min. :0.000 ## 1st Qu.:0.000 ## Median :0.000 ## Mean :0.454 ## 3rd Qu.:1.000 ## Max. :1.000 # Other approaches psych::describe(dat1) ## vars n mean sd median trimmed mad min ## Pregnancies 1 768 3.85 3.37 3.00 3.46 2.97 0.00 ## Glucose 2 768 120.89 31.97 117.00 119.38 29.65 0.00 ## BloodPressure 3 768 69.11 19.36 72.00 71.36 11.86 0.00 ## SkinThickness 4 768 20.54 15.95 23.00 19.94 17.79 0.00 ## Insulin 5 768 79.80 115.24 30.50 56.75 45.22 0.00 ## BMI 6 768 31.99 7.88 32.00 31.96 6.82 0.00 ## DiabetesPedigreeFunction 7 768 0.47 0.33 0.37 0.42 0.25 0.08 ## Age 8 768 33.24 11.76 29.00 31.54 10.38 21.00 ## Outcome 9 768 0.35 0.48 0.00 0.31 0.00 0.00 ## Glucose_hi 10 768 0.45 0.50 0.00 0.44 0.00 0.00 ## max range skew kurtosis se ## Pregnancies 17.00 17.00 0.90 0.14 0.12 ## Glucose 199.00 199.00 0.17 0.62 1.15 ## BloodPressure 122.00 122.00 -1.84 5.12 0.70 ## SkinThickness 99.00 99.00 0.11 -0.53 0.58 ## Insulin 846.00 846.00 2.26 7.13 4.16 ## BMI 67.10 67.10 -0.43 3.24 0.28 ## DiabetesPedigreeFunction 2.42 2.34 1.91 5.53 0.01 ## Age 81.00 60.00 1.13 0.62 0.42 ## Outcome 1.00 1.00 0.63 -1.60 0.02 ## Glucose_hi 1.00 1.00 0.18 -1.97 0.02 broom::tidy(dat1) ## # A tibble: 10 x 13 ## column n mean sd median trimmed mad min max range ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Pregn… 768 3.85 3.37 3 3.46 2 0 17 17 ## 2 Gluco… 768 121. 32.0 117 119. 20 0 199 199 ## 3 Blood… 768 69.1 19.4 72 71.4 8 0 122 122 ## 4 SkinT… 768 20.5 16.0 23 19.9 12 0 99 99 ## 5 Insul… 768 79.8 115. 30.5 56.7 30.5 0 846 846 ## 6 BMI 768 32.0 7.88 32 32.0 4.6 0 67.1 67.1 ## 7 Diabe… 768 0.472 0.331 0.372 0.422 0.168 0.078 2.42 2.34 ## 8 Age 768 33.2 11.8 29 31.5 7 21 81 60 ## 9 Outco… 768 0.349 0.477 0 0.312 0 0 1 1 ## 10 Gluco… 768 0.454 0.498 0 0.443 0 0 1 1 ## # … with 3 more variables: skew &lt;dbl&gt;, kurtosis &lt;dbl&gt;, se &lt;dbl&gt; 1.8 RStudio - The IDE for R RStudio is an integrated development environment (IDE) for R. For overall convenience, flexibility, educational resources, and ongoing development it is in my opinion an unparalleled environment for working in R. It offers a multi-pane console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, project and workspace management. There are many tools within RStudio that facilitate coding including numerous shortcuts which accessed from a drop down menu within RStudio and can be found here. Several shortcuts that I find most helpful are listed below. Table: Very useful RStudio shortcuts | Command | Windows | Mac | |---------------------------------- |--------------------------------------------- |----------------- | | Assignment operator | Alt + - | Opt + - | | Commenting &amp; Uncommenting Code | Ctrl + Shift + C | Cmd + Shift + C | | Add the Pipe %&gt;% | Ctrl + Shift + M | Cmd + Shift + M | | Keyboard Shortcut Cheat Sheet | Alt + Shift + K | Opt + Shift + K | | Move cursor beginning of line | Home | Cmd+Left | | Move cursor to end of line | End | Cmd+Right | When using RStudio, it generally most helpful to begin by creating a New Project from theFiledrop down menu. As you will soon appreciate this has definitely file management advantages. For individual files, I find it most useful to create individual RMarkdown documents. For this book, each chapter is a separateRmdfile. These files have the advantage of being able to combine free text andRcode chunks which via a synthesis of themarkdownlanguage andPandoc` allows the output to be on the format of your choice (html, LaTex/pdf, WORD). 1.9 R - More than a statistical program R is much more than a mere statistical program. It is a complete programming language which while highly advantageous does result in a non trivial learning curve. One of the most outstanding attributes of R is the ability to produce publication quality data visualizations with either base R or within the tidyverse universe by using ggplot2 (see next chapter). Interactive graphics can also be easily produced. To appreciate the range of graphical activities possible, here is a self portrait drawn by R. The code for this may be found here. Figure 1.2: Self portrait Some beautiful art and the accompanying R code can be found here Figure 1.3: R art 1.10 R - General Public License R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3. For more information about these matters see http://www.gnu.org/licenses/. References "],["vis.html", "Chapter 2 Exploratory Data Analysis - Data visualization 2.1 Introduction 2.2 Why is data visualization important? 2.3 Creating an effective and professional appearing graph 2.4 Graphical choices 2.5 Example - A modern version of the “Ghost Map”", " Chapter 2 Exploratory Data Analysis - Data visualization 2.0.1 R packages required for this chapter library(dslabs) library(ggplot2) library(dplyr) library(magrittr) library(gganimate) library(scales) library(transformr) library(dslabs) # contains gapminder data set, also found in gapminder package library(HistData) library(maptools) library(rgdal) library(leaflet) library(ggmap) 2.1 Introduction There is a modern tendency for researchers to immediately present their results in the most complex and sophisticated manner. This is often seen as proof of their respectability and importance. However, a step wise approach that begins by presenting data in its rawest form of Tables and Graphs is both more logical and beneficial to understanding. In fact, there is are several historical examples of excellent visualizations involving epidemiology. Although remembered as the mother of modern nursing, Florence Nightingale was an accomplished statistician too being the first female fellow of the Statistical Society of London (now Royal Statistical Society)(The Economist 2013). She was particularly innovative in presenting data visually as by this example published in her 1858 monograph, “Notes on matters affecting the health, efficiency and hospital administration of the British army.” The chart displays the causes of the deaths of soldiers during the Crimean war, divided into three categories: “Preventible or Mitigable Zymotic Diseases” (infectious diseases, including cholera and dysentery, coloured in blue), “wounds” (red) and “all other causes” (black). As with today’s pie charts, the area of each wedge is proportional to the figure it stands for, but it is the radius of each slice (the distance from the common centre to the outer edge) rather than the angle that is altered to achieve this. Her principal message—that even during periods of heavy fighting, such as November 1854, far more soldiers died from infection than from wounds—can be seen at a glance. It seems a fair bet that her talents as a data scientist contributed to her successful introduction of medical advances in military hospitals. Figure 1.1: Classic epidemiology visualizations - Nightingale’s Rose Charles Minard’s 1869 publication of a flow map of Napoleon’s Russian 1812 campaign has been called best statistical graphic ever drawn by Edward Tufte who is a preeminent 21st century statistician and pioneer in the field of data visualization. The figure shows 6 six types of data in two dimensions: the number of Napoleon’s troops; the distance traveled; temperature; latitude and longitude; direction of travel; and location relative to specific dates. The chart tells with painful clarity the atrocious losses associated with this campaign as an initial force of 422,000 saw only only 10,000 return. On the retrurn trip 50% of the forces were lost while crossing the Bérézina river under heavy attack. “C’est la Bérézina” is a French expression used to describe a total disaster. Figure 2.1: Classic epidemiology visualizations - Napoleon’s Russian 1812 Campaign 2.2 Why is data visualization important? Specifically, data visualizations can help * understand your data * understand basic concepts * emphasize a message * build trust with your audience * clarify your story for others * inform / influence their decisions * poor data visualizations can do the opposite! As an example of an uninspired visualization, consider the frequently observed bar plot which is a regular item in scientific publications. Contrast the lack of information provided by the commonly used bar plot format (Figure 2.2) (avoiding any association with inferiority simply because it was created with Excel) with the additional data insights provided by simple histograms and box plots (Figure 2.3). Figure 2.2: Poor presentation - Bar plot Figure 2.3: Improved presentation - histograms and box plots As another example of an uninspired plot, consider the pie chart which has been rightly criticized as being difficult to understand as we are effectively being asked to compared angles and the area subtended by them rather than simply making linear comparisons as in barplots. Notwithstanding the presence of Steve Jobs with the pie chart, which visualization is easiest to understand? Figure 2.4: Pie chart (with Steve Jobs) Figure 2.5: Bar plot (without Steve Jobs) In contrast, if you have any doubts about the power of excellent visualizations, you must watch this video by Hans Rosling. Leaving aside his fabulous and infectious enthusiasm, (who wouldn’t want to be a global health data scientist after watching this), the knowledge transfer from his data visualizations is simply amazing. 2.3 Creating an effective and professional appearing graph Here is a static image from Rosling’s video. It certainly is of professional quality. Can we reproduce it? Figure 2.6: Gapminder data If the first plot we produce is of this quality, it will hopefully inspire us to realize what we can accomplish with data visualizations using R. While this may rightly not completely dispel the notion that a definitive learning curve is associated withR, it is certainly suggests it can be manageable. Here is the code that reproduces the graph. dat &lt;- gapminder %&gt;% filter(year == 2007) un_graph &lt;- ggplot(dat, aes(x = gdp/population, y = life_expectancy)) + geom_point(aes(size = population, color = continent), alpha = 0.5) + stat_smooth(formula = y ~ log10(x), se = FALSE, size = 0.5, color = &quot;black&quot;, method = &#39;gam&#39;, linetype=&quot;dotted&quot;) + xlab(&quot;GDP per capita&quot;) + ylab(&quot;Life expectancy at birth&quot;) + scale_size_area(guide = FALSE, max_size = 15) + scale_x_continuous(labels = scales::dollar, breaks = seq(0, 50000, by = 10000)) + ggtitle(&quot;Gapminder data from 2007&quot;) + labs(caption = &quot;Data from dslabs library&quot;) + scale_color_brewer(name = &quot;&quot;, palette = &quot;Dark2&quot;) + theme_minimal(base_size = 12, base_family = &quot;Georgia&quot;) un_graph ## Warning: Removed 4 rows containing non-finite values (stat_smooth). ## Warning: Removed 4 rows containing missing values (geom_point). # to save your figure # ggsave(&quot;files/un_graph.png&quot;) The code can be understood as follows; 1. We load the data which is available in both the R Gapminder and dslabs libraries (dslabs more complete). 2. As data exists every 5 years from 1952, we have selectrd only 2007 data to match the static plot 3. Graphing will use ggplot function from the ggplot2 library 4. The ggplot function first requires a data argument (in the form of a data frame) 5. The next argument is aes for aesthetics and identifies the x and y variable names from the data frame 6. In R any task such as identifying variable names can usually be accomplished by several ways (e.g. names(gapminder), str(gapminder), glimpse(gapminder), head(gapminder)) 7. Output is layered onto the graphical surface beginning here with the data points (geom_points) with color assigned according to continent, followed by a smoothed regression line (stat_smooth) 8. Next purely aesthetic details such as a title, legends and a theme are added. As a beginner, and even as a more advanced user, it is not necessary to memorize all the possible formatting options as quick and accessible online resources are available to add these final refinements to your graphs (see Using the gganimate package, a fully animated graph can be made with the code below ### animated graph un_plot &lt;- ggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(size = pop, color = continent), alpha = 0.5) + stat_smooth(formula = y ~ log10(x), se = FALSE, size = 0.5, color = &quot;black&quot;, method = &#39;gam&#39;, linetype=&quot;dotted&quot;) + xlab(&quot;GDP per capita&quot;) + ylab(&quot;Life expectancy at birth&quot;) + scale_size_area(guide = FALSE, max_size = 15) + scale_x_continuous(labels = scales::dollar) + ggtitle(&quot;Gapminder data from 2007&quot;) + labs(caption = &quot;Data from gapminder library&quot;) + scale_color_brewer(name = &quot;&quot;, palette = &quot;Dark2&quot;) + theme_minimal(base_size = 12, base_family = &quot;Georgia&quot;) + # gganimate code ggtitle(&quot;{frame_time}&quot;) + transition_time(year) + ease_aes(&quot;linear&quot;) + enter_fade() + exit_fade() animate(un_plot) While we are considering this international data source, let’s see how you do with a simple quiz. Q1. Which countries in each pair have the highest child mortality (2015)? Q2. Which pairs of countries are most similar? * Sri Lanka or Turkey * Poland or South Korea * Malaysia or Russia * Pakistan or Vietnam * Thailand or South Africa Now as it is unlikely that you have actual knowledge of country specific mortality data, most individuals will attempt to employ some heuristic to help answer the question. One such heuristic would say “European countries lower child mortality than non-European”. Following that heuristic, our answers might be: Sri Lanka, South Korea, Malaysia with Pakistan or Vietnam and Thailand or South Africa as non-European countries being the most similar. Fortunately we can do better by analyzing the gapminder dataset ## A bit of code library(dslabs) #source of gapminder data data(gapminder) # write a simple function for the comparison between 2 countries compare_2 &lt;- function(x,y){ gapminder %&gt;% filter(year == &quot;2015&quot; &amp; country %in% c(x, y)) %&gt;% select(country, infant_mortality) } compare_2(&quot;Sri Lanka&quot;, &quot;Turkey&quot;) ## country infant_mortality ## 1 Sri Lanka 8.4 ## 2 Turkey 11.6 compare_2(&quot;Poland&quot;, &quot;South Korea&quot;) ## country infant_mortality ## 1 South Korea 2.9 ## 2 Poland 4.5 compare_2(&quot;Malaysia&quot;, &quot;Russia&quot;) ## country infant_mortality ## 1 Malaysia 6.0 ## 2 Russia 8.2 compare_2(&quot;Pakistan&quot;, &quot;Vietnam&quot;) ## country infant_mortality ## 1 Pakistan 65.8 ## 2 Vietnam 17.3 compare_2(&quot;Thailand&quot;, &quot;South Africa&quot;) ## country infant_mortality ## 1 South Africa 33.6 ## 2 Thailand 10.5 If we were merely ignorant we might have expected to get 2.5 answers right but it seems if we followed our heuristic we would have got 0 right answers implying we are more than simply ignorant but actually misinformed! We can use the same dataset to query our heuristic about better survival in European countries. gapminder %&gt;% filter(continent == &quot;Asia&quot; | continent == &quot;Europe&quot;) %&gt;% filter(year %in% c(1962, 1980, 1990, 2012)) %&gt;% ggplot(aes(fertility, infant_mortality, color= continent)) + facet_grid(cols = vars(year)) + geom_point() + ylim(0,200) + ggtitle(&quot;Infant mortality over time&quot;) + theme_bw() ## Warning: Removed 53 rows containing missing values (geom_point). gapminder %&gt;% filter(continent == &quot;Asia&quot; | continent == &quot;Europe&quot;) %&gt;% filter(year %in% c(1962, 1980, 1990, 2012)) %&gt;% ggplot(aes(fertility, life_expectancy, color= continent)) + facet_grid(cols = vars(year)) + geom_point() + ggtitle(&quot;Life expectancy over time&quot;) + theme_bw() These graphs show that the heuristic of improved infant and overall life expectancy among European countries was true 60 years ago but is no longer today’s truth. This underlines the need to continually question our baseline assumptions. 2.4 Graphical choices Visualizations play an important role in understanding the data, exploring the data, and interpreting the data. The most common visualization methods are: * Histograms * Scatterplots * Boxplots * Survival curves (Kaplan Meier) * Heatmaps and many more A graph can provide a quick interpretation of the data and orient more in depth analyses. Figure 2.7: Data visualization -&gt; Quick understanding of data Consider this story from the New York Times about the scoring of high school leaving exams where 65 is the passing grade. A histogram of the grades shows important abnormalities with 5 times more students scoring exactly 65 than those scoring between 61-64, leaving a suspicion that grading may not be completely unbiased. Simply reporting the mean, median, standard deviations, percentage passing or any other numerical summary would not have detected the strangeness of the data. Figure 2.8: Histogram of grades Graphs can also be combined to provide further insights into the data. The following is a graph from the excellent book The Art of Statistics by David Speigelhalter that shows the gender and timing of the 215 confirmed victims of the mass murderer, Dr. Harold Shipman. Figure 2.9: Murder victims Here a basic scatter plot has been combined with marginal histograms. This enables a quick appreciation that the victims were 1. more often women 2. mostly older but some younger victims in more recent times 3. a gap occured around 1992 4. there were increased murders in later years Fortunately, most datasets that we will examine are not this gruesome. 2.5 Example - A modern version of the “Ghost Map” Dr. John Snow (1813-1858) was a famous British physician and is widely recognized as a legendary figure in the history of public health and a leading pioneer in the development of anesthesia. As a leading advocate of both anesthesia and hygienic practices in medicine, he not only experimented with ether and chloroform but also designed a mask and method how to administer it. He personally administered chloroform to Queen Victoria during the births of her eighth and ninth children, in 1853 and 1857, which assured a growing public acceptance of the use of anesthetics during childbirth. In the realm of public health, Snow is especially known for his work on the transmission of cholera during the cholera epidemics that occurred regularly in 19th century London, generally believed to be brought in by merchant marine trade, exacerbated by overcrowding (London 1850 population exceeded 2 million) and unsanitary streets. An excellent book covering Snow’s actions in identifying the source of the 1854 Soho cholera outbreak is masterfully told in The Ghost Map by Steven Johnson. The book illustrates the importance of “shoe leather” epidemiology in establishing the water transmission theory of cholera and the difficulties in overcoming the confirmation bias supporting the prevalent miasma theory of transmission. Final recognition of the correctness of Snow’s theory was hard earned as his initial research on cholera had been criticized in medical reviews, including the Lancet. A philistine might say this is but an early example of an ongoing problem with peer review publishing in august medical journals (more on modern contemporary examples will be forthcoming in later chapters). John Snow is now recognized as one of the founders of modern epidemiology and in later publications Snow made use of data visualization to strengthen his message. Snow’s original geographical mapping of the data is not available but one that he drew about a year later in 1855 depicts the deaths and is referred to as The Ghost Map. More details on the history of the 1854 Broad Street cholera outbreak are found here. Here is an initial plot as displayed on John Snow’s original map. Figure 2.10: Classic epidemiology visualizations - John Snow’s Cholera Broad Street Ghost Map When many points are associated with a single street address, they are “stacked” in a line away from the street so that they are more easily visualized. This dataset is found in the HistData package and has 578 observations on the following 3 variables, giving the address of a person who died from cholera. 2.5.1 Using modern Geographic Information System mapping Can download a zip file with GIS layers relating to John Snow’s 1854 investigation of the Cholera outbreak in London. This file contains a number of GIS layers created from Snow’s original map which allow analyses to be conducted on the data in modern GIS systems. ## Download the zip archive of geographical information download.file(url = &quot;http://www.rtwilson.com/downloads/SnowGIS_v2.zip&quot;, destfile = &quot;datasets/SnowGIS_v2.zip&quot;) ## Unzip unzip(zipfile = &quot;datasets/SnowGIS_v2.zip&quot;, exdir =&quot;datasets&quot;) ## List files in the unzipped folder -&gt; dir(path = &quot;./datasets/SnowGIS&quot;) # register_google(key = &quot;Put your API key here&quot;) # my API is hidden GDAL &lt;- GDALinfo(&quot;./datasets/SnowGIS/OSMap.tif&quot;) OSMap &lt;- readGDAL(&quot;./datasets/SnowGIS/OSMap_Grayscale.tif&quot;) par(mar = c(0,0,0,0)) # image(OSMap, col = grey(1:500/1000)) modern plot not shown here getinfo.shape(&quot;./datasets/SnowGIS/Cholera_Deaths.shp&quot;) map1 &lt;- get_map(c(-.137,51.513), zoom=16) london_main &lt;- ggmap(map1) + ggtitle(&quot;Modern Google style Ghost map&quot;) ### Assigning the Cordinates of Deaths &amp; Pumps Variables from the Dataset deaths &lt;- readShapePoints(&quot;datasets/SnowGIS/Cholera_Deaths&quot;) ## Warning: readShapePoints is deprecated; use rgdal::readOGR or sf::st_read pumps &lt;- readShapePoints(&quot;datasets/SnowGIS/Pumps&quot;) ## Warning: readShapePoints is deprecated; use rgdal::readOGR or sf::st_read df_deaths &lt;- data.frame(deaths@coords) df_pumps &lt;- data.frame(pumps@coords) tmp &lt;- rbind(df_deaths, df_pumps) tmp$type &lt;- c(rep(&#39;death&#39;, times=dim(df_deaths)[1]), rep(&#39;pump&#39;, times=dim(df_pumps)[1])) coordinates(df_deaths)=~coords.x1+coords.x2 proj4string(df_deaths)=CRS(&quot;+init=epsg:27700&quot;) ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;, prefer_proj = ## prefer_proj): Discarded datum OSGB_1936 in Proj4 definition df_deaths = spTransform(df_deaths,CRS(&quot;+proj=longlat +datum=WGS84&quot;)) df=data.frame(df_deaths@coords) lng=df$coords.x1 lat=df$coords.x2 coordinates(tmp)=~coords.x1+coords.x2 proj4string(tmp)=CRS(&quot;+init=epsg:27700&quot;) ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;, prefer_proj = ## prefer_proj): Discarded datum OSGB_1936 in Proj4 definition tmp = spTransform(tmp, CRS(&quot;+proj=longlat +datum=WGS84&quot;)) tmp &lt;- data.frame(tmp@coords, type=tmp@data$type) library(patchwork) snow.plot1 &lt;- london_main + geom_point(mapping=aes(x=coords.x1, y=coords.x2, col=type, size = 1), data=tmp) + annotate(&quot;text&quot;, x = -.1361, y = 51.513, label = &quot;X\\n Broad Street pump&quot;, colour=&quot;black&quot;, size = 4) + labs(subtitle = &quot;Regular plot&quot;) snow.plot1 snow.plot2 &lt;- london_main + geom_density2d(data = tmp[tmp$type ==&quot;death&quot;, ], aes(x = coords.x1, y = coords.x2), size = 0.6) + annotate(&quot;text&quot;, x = -.1361, y = 51.513, label = &quot;X\\n Broad Street pump&quot;, colour=&quot;black&quot;, size = 4) + labs(subtitle = &quot;Density plot&quot;) snow.plot2 References "],["meas.html", "Chapter 3 Contingency tables, measures of association &amp; R packages 3.1 Proportions - One sample \\(\\chi^2\\) tests 3.2 Contingency tables 3.3 Some basic defintions 3.4 Examples using base R 3.5 Examples using R epidemiology packages", " Chapter 3 Contingency tables, measures of association &amp; R packages 3.0.1 R packages required for this chapter library(knitr) library(kableExtra) library(tidyverse) # specifically dplyr and ggplot2 library(epiR) library(epibasix) library(epitools) 3.1 Proportions - One sample \\(\\chi^2\\) tests Suppose we have the following data and want to test if the hypothesis that \\(\\pi\\) = 0.80 is true. In this simple example, the base R chisq.test function is more than adequate. Success Failure Population 60 40 # one sample chi sq test x &lt;- c(60,40) chisq.test(x, p = c(.8, .2)) ## ## Chi-squared test for given probabilities ## ## data: x ## X-squared = 25, df = 1, p-value = 6e-07 For small sample sizes (n &lt; 5), chisq.test is not valid. A famous example of a small data set is Fisher’s lady tasting tea example. In this experiment, a lady claims she can tell if milk is added to the cup before or after the tea. suppose there are 8 cups, 4 with milk added first and 4 with milk added after the tea. How unlikely would it be if she could identify correctly the 8 cups? This can be calculated by hand using the hypergeometric distribution or using fisher.test. Tea &lt;- matrix(c(4, 0, 0, 4), nrow = 2, dimnames = list(Guess = c(&quot;Milk&quot;, &quot;Tea&quot;), Truth = c(&quot;Milk&quot;, &quot;Tea&quot;))) temp &lt;- fisher.test(Tea, alternative = &quot;greater&quot;) Tea ## Truth ## Guess Milk Tea ## Milk 4 0 ## Tea 0 4 temp ## ## Fisher&#39;s Exact Test for Count Data ## ## data: Tea ## p-value = 0.01 ## alternative hypothesis: true odds ratio is greater than 1 ## 95 percent confidence interval: ## 2.004 Inf ## sample estimates: ## odds ratio ## Inf # more precise p value temp$p.value ## [1] 0.01429 There is a 1.4% chance of this occurring. More commonly we will have larger 2X2 datasets, or more generally nXm datasets to investigate and these may be helpfully presented in contingency tables. 3.2 Contingency tables Begin by reading in the heart.csv dataset where the outcome status is the fstat variable. Details about the other variables can be obtained with str(). It is often of interest to initially explore any categorical exposure and outcome create a contingency table of fstat (alive / dead) versus gender (men=0, women=1). heart &lt;- read.csv(&quot;data/heart.csv&quot;, header = TRUE) # create gender factor variable heart$gender &lt;- factor(heart$gender, labels = c(&quot;men&quot;, &quot;women&quot;)) # Create a 2-way contingency table of gender vs outcome table(heart$gender,heart$fstat ) ## ## alive dead ## men 189 111 ## women 96 104 As discussed in Chapter 2, graphical displays are often helpful and informative. # Create side-by-side barchart outcome and gender ggplot(heart, aes(x = gender, fill = fstat)) + geom_bar(position = &quot;dodge&quot;) + #position = &quot;dodge&quot;, to have a side-by-side (i.e. not stacked) barchart theme_bw() There are more males than females in this dataset. Total male and female deaths are about equal but better survival rates in males. Marginals can be easily obtained as can statistical testing with chisq.test or fisher.test tab &lt;- table(heart$gender,heart$fstat ) # marginals rowSums(tab) ## men women ## 300 200 colSums(tab) ## alive dead ## 285 215 paste(&quot;Probabilities conditional on columns&quot;) ## [1] &quot;Probabilities conditional on columns&quot; prop.table(tab, 2) ## ## alive dead ## men 0.6632 0.5163 ## women 0.3368 0.4837 paste(&quot;\\nProbabilities conditional on rows&quot;) ## [1] &quot;\\nProbabilities conditional on rows&quot; prop.table(tab, 1) ## ## alive dead ## men 0.63 0.37 ## women 0.48 0.52 # statistical testing chisq.test(tab) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: tab ## X-squared = 10, df = 1, p-value = 0.001 fisher.test(tab) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: tab ## p-value = 0.001 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.262 2.696 ## sample estimates: ## odds ratio ## 1.842 Now while the base R chisq.test function provides the OR, 95% CI, and a p value, we will show in section 3.5 that the use of epidemiologic specific R packages allows the use of more customized and detailed functions. Suppose we are now interested in the gender outcomes association but according to the presence or absence of pre-existing cardiac disease. Adding this third variable, cvd is straightforward but the limits of this approach are readily appreciated as the number of variables increases. table(heart$gender, heart$fstat, heart$cvd) ## , , = no ## ## ## alive dead ## men 65 24 ## women 15 21 ## ## , , = yes ## ## ## alive dead ## men 124 87 ## women 81 83 # Plot of alignment broken down by gender ggplot(heart, aes(x = gender,fill = fstat )) + geom_bar() + facet_wrap(~ cvd) + theme_bw() 3.3 Some basic defintions Cumulative incidence —proportion of new cases developing in an initial disease free population during a given risk period. Incidence time —time span from 0 to the time of outcome/event/failure/occurrence Person-time —length of time that each individual was in the population at risk of the event Total person-time at risk - sum of all individual person-times Incidence rate —number of new cases of disease divided by person-time over the period Incidence proportion or cumulative incidence —with complete cohort follow-up, it is the proportion of people who become cases among those in the population at the start of the interval. Can also be considered a measure of average risk Comparison of cumulative incidence and incidence rate Cumulative Incidence Cumulative Incidence Incidence rate Incidence rate FU complete FU incomplete FU complete FU incomplete Numerator cases Kaplan Meier Lfe Table Number cases Number cases Denominator population Kaplan Meier Lfe Table Person - time Average population Units no units no units Time-1 Time-1 Range 0 to 1 0 to 1 0 to infinity 0 to infinity Synonyms Probability Proportion Probability Proportion Incidence density Incidence density Risk is a general term but often refers to cumulative incidence (Q) but other interpretations including instantaneous risk (hazard) and risk at a given time point (prevalence). Incidence rate and cumulative incidence proportion are longitudinal measures. In contrast prevalence measures are cross-sectional. The numerator of cumulative incidence and incidence rates are the number of cases while the denominator is proportional to the size (counts or person time) of the population from whioch the cases are derived. Numerator and denominator must cover the same population and the same time period. 3.3.1 Relative comparative measures Generic name “relative risk” (RR) comparing occurrences between exposed (1) and unexposed (0) groups can refer to * Incidence rate ratio IR = I1 / I0 (the most commonly used comparative measure) * Incidence proportion ratio IPR = Q1 / Q0 * Incidence odds ratio IOR = [Q1/(1 − Q1)] / [Q0/(1 − Q0)] * Prevalence ratio PR = P1/P0 * Prevalence odds ratio POR = [P1/(1 − P1)] / [P0/(1 − P0)] 3.3.2 Absolute comparative measures Generic term “excess risk” or “risk difference” (RD) between exposed and unexposed can refer to * Incidence rate difference ID = I1 − I0 * Incidence proportion difference IPD = Q1 − Q0 * Prevalence difference PD = P1 − P0 Ratios – most often employed to describe the biological strength of the exposure Differences – absolute differences better inform about public health importance 3.3.3 Attributal measures -Attributable fraction (excess fraction or attributable risk) is a measures of potential impact. \\[AF = \\dfrac{I_{1} - I_{0}}{I_{1}} = \\dfrac{RR-1}{RR}\\] This measure estimates the fraction out of all new cases of disease among those exposed, which are attributable to (or “caused” by) the exposure itself, and which thus could be avoided if the exposure were absent. This represents the biological impact of exposure and is represented diagrammatically as where AF fraction = black area out of total black + gray area. -Prevented fraction When the incidence in exposed is lower, we define the prevented fraction as \\[AF = \\dfrac{I_{0} - I_{1}}{I_{0}} = 1 - RR\\] -Population attributable (excess) fraction (PAF) addresses the impact of exposure on the population level and also depends \\(p_E\\), the proportion of exposed in the population. \\[\\begin{equation} PAF = I - I_{0} \\tag{3.1} \\end{equation}\\] where \\(I\\) = population incidence that can be expressed as a weighted average among exposed and unexposed as follows \\(I = p_{E}*I_{1} + (1-p_{E})*I_{0}\\) and substituting into Equation (3.1) gives \\[\\begin{equation} PAF = \\dfrac{I - I_{0}}{I} = \\dfrac{p_{E} * (RR-1)}{1 + p_{E}* (RR-1) } \\tag{3.2} \\end{equation}\\] Important: Use the crude and NOT adjusted RR in the above PAF formula that uses the fraction of the entire population exposed Diagrammatically represented as where PAF fraction = black area out of total black + gray area. The PAF is only a simple fraction derived from the arithmetic manipulation of probabilities. As with other measures in public health, how this fraction is interpreted is key. For PAF to be important in the discussion of the public health consequences of intervening to reduce the prevalence of risk exposures, we need to be sure of - the causal model - absence or control of confounding - understanding of multiple exposures &amp; their combinations - existence of a feasible intervention with no untoward effects R packages for AF estimation include epiR, attribrisk, paf and AF. AF can handle confounders for different research designs, including survival data, matched case/control and clustered data. 3.3.4 Standardization Sometimes the exposure disease relationship is distorted by another variable and stratification is the most transparent manner of dealing with this confounding. Combining this strata is most often used for age adjustment.Other more advanced techniques for dealing with confounding will be discussed in later chapters. - Direct adjustment by standardization \\(= \\sum_{k=1}^K weight_k * rate_k /\\) sum of weights is a weighted average of stratum-specific rates for each group and applies them to a common standard, but often arbitrary, population, thereby removing differences due to distribution of the population - Indirect adjustment is performed whenever the stratum-specific incidence risk estimates are either unknown or unreliable. In this case external stratum specific rates are applied. 3.4 Examples using base R 3.4.1 Example 1 - basic incidence measures Consider the follow-up of a small cohort of 10 subjects. # Create small data set set.seed(1234) data &lt;- data.frame( x=factor(c(1:10)), y=abs(runif(10, 1,24)), cen=factor(c(&quot;n&quot;,&quot;y&quot;,&quot;n&quot;, &quot;y&quot;,&quot;n&quot;,&quot;n&quot;,&quot;y&quot;,&quot;n&quot;,&quot;y&quot;,&quot;n&quot;), labels = c(&quot;died&quot;, &quot;censored&quot;)) ) # plot of follow-up time p &lt;- ggplot(data, aes(x=x, y=y, color=cen)) + geom_segment( aes(x=x, xend=x, y=0, yend=y ) ) + geom_point( color=ifelse(data$cen == &quot;died&quot;, &quot;orange&quot;, &quot;blue&quot;), size=ifelse(data$cen == &quot;died&quot;, 5, 2) ) + theme_bw() + coord_flip() + xlab(&quot;subject&quot;) + scale_y_continuous(name=&quot;Follow-up (months)&quot;, breaks = seq(0, 24, 3), limits=c(0, 24)) + ggtitle(&quot;Horizontal plot of follow-up time&quot;) + labs(color = &quot;Status&quot;) p The incidence rate at the end of the follow-up (21 months) = total number of deaths / total person time follow-up options(scipen = 1, digits = 2) ir_m &lt;- sum(data$cen==&quot;died&quot;) / sum(data$y) The incidence rate at the end of the follow-up (21 months) = 0.05 / person month or 4.9 / 100 person months The incidence rate at the end of the follow-up could also be expressed in person years = 0.59 / person year or 58.76 / 100 person years One could also calculate the incidence rate for the first 12 months. options(scipen = 1, digits = 2) deaths_1 &lt;- data %&gt;% filter( y&lt;= 12) %&gt;% summarise(n=n(), months=sum(y)) In this time period there were 3 deaths in 11.18 months of follow-up. There were also 7 subjects who completed the 12 months follow-up with no deaths. So the total follow-up time = 95.18 Therefore the incidence rate at 12 months = 0.03 / person month or 3.15 / 100 person months. What is the cumulative incidence over 21 months? On the surface, straightforward calculation of number of deaths / initial population = 0.6. However this ignores the censoring. The incidence proportions in the presence of censoring can be estimated by assuming a constant rates. \\[Q = 1 − exp(−I × ∆) = 1- exp(-0.049 * 21) = 0.64\\] With dynamic study population individual follow-up times are variable and difficult to measure accurately such that a common approximation is to use the mid-population average of the initial and final populations multiplied by the follow-up time. 3.4.2 Example 2 - rare disease For rare disease, the cumulative incidence, rate incidence and incident odds ratio are very similar. Consider the following data; options(digits = 5) dt &lt;- data.frame(Yes= c(4000,30, 7970), No=c(16000,60, 31940), row.names=c(&quot;No. initally at risk&quot;, &quot;No. cases&quot;, &quot;Person years at risk&quot;)) kbl(dt) %&gt;% kable_paper() %&gt;% add_header_above(c(&quot; &quot;, &quot;Exposure&quot; = 2)) Exposure Yes No No. initally at risk 4000 16000 No. cases 30 60 Person years at risk 7970 31940 cum_inc &lt;- (dt[2,1]/dt[1,1]) / (dt[2,2]/dt[1,2]) # 30/4000 / 60/16000 rate_ratio &lt;- (dt[2,1]/dt[3,1]) / (dt[2,2]/dt[3,2]) odds_ratio &lt;- (dt[2,1]/(dt[1,1]-dt[2,1])) / (dt[2,2]/(dt[1,2]-dt[2,2])) This gives a cumulative incidence, rate incidence and incident odds ratio of 2 , 2.0038 , 2.0076, respectively 3.4.3 Example 3 - Standardization Consider the following cases and populations at risk from 2 different cities Table 3.1: Cases and numbers at risk age cases at_risk city &lt;40 20 100 Pop1 &gt;40 100 200 Pop1 &lt;40 40 400 Pop2 &gt;40 80 200 Pop2 The crude risk ratio (Population 1 : Population 2) is 2 but there is severe unbalancing in the age distributions of the 2 populations and age influences the outcome. # stratum specific rates rate1 &lt;- dat[1:2,&quot;cases&quot;] /dat[1:2, &quot;at_risk&quot;] rate2 &lt;- dat[3:4,&quot;cases&quot;] /dat[3:4, &quot;at_risk&quot;] std.pop &lt;- c(500, 100) # standard young population # adjusted risk ratio std.pop1 &lt;- rate1*std.pop; std.pop2 &lt;- rate2*std.pop rr_std_y &lt;- sum(std.pop1) / sum(std.pop2) std.pop &lt;- c(100, 500) # standard older population # adjusted risk ratio std.pop2 &lt;- rate1*std.pop; std.pop2 &lt;- rate2*std.pop rr_std_o &lt;- sum(std.pop1) / sum(std.pop2) Standardizing the age distributions (500, 100), we see that Population 2 still has an increased risk, 1.66667, but part of the crude difference was due to the confounding effect of age. Standardizing to an older population (100,500) leads to a smaller relative risk, 0.71429. The calculations could be incorporated into a user defined function. std.pop &lt;- c(100, 500) std.direct &lt;- function(x1,y1,x2,y2,std.pop){ rate1 &lt;- x1/y1; rate2 &lt;- x2/y2 std.pop1 &lt;- rate1*std.pop; std.pop2 &lt;- rate2*std.pop sum(std.pop1) / sum(std.pop2) } std.direct(dat[1:2,2], dat[1:2,3], dat[3:4,2], dat[3:4,3], std.pop) ## [1] 1.2857 3.5 Examples using R epidemiology packages 3.5.1 Example 4 Standardization Continuing with the above example, rather than writing your own function, one could use epi.directadj from the epiR package. Care must be taken to provide the arguments as matrices. # preprocessing function arguments to matrices obs &lt;- matrix(dat$cases, nrow = 2, byrow = TRUE, dimnames = list(c(&quot;Pop1&quot;,&quot;Pop2&quot;), c(&quot;&lt;40&quot;,&quot;&gt;40&quot;))) tar &lt;- matrix(dat$at_risk, nrow = 2, byrow = TRUE, dimnames = list(c(&quot;Pop1&quot;,&quot;Pop2&quot;), c(&quot;&lt;40&quot;,&quot;&gt;40&quot;))) std &lt;- matrix(data = c(500,100), nrow = 1, byrow = TRUE, dimnames = list(&quot;&quot;, c(&quot;&lt;40&quot;,&quot;&gt;40&quot;))) ans &lt;- epi.directadj(obs, tar, std, units = 1, conf.level = 0.95) rr &lt;- ans$adj.strata$est[1] / ans$adj.strata$est[2] ans ## $crude ## strata cov est lower upper ## 1 Pop1 &lt;40 0.2 0.122165 0.30888 ## 2 Pop2 &lt;40 0.1 0.071441 0.13617 ## 3 Pop1 &gt;40 0.5 0.406820 0.60813 ## 4 Pop2 &gt;40 0.4 0.317175 0.49783 ## ## $crude.strata ## strata est lower upper ## 1 Pop1 0.4 0.33164 0.47830 ## 2 Pop2 0.2 0.16582 0.23915 ## ## $adj.strata ## strata est lower upper ## 1 Pop1 0.25 0.18082 0.34038 ## 2 Pop2 0.15 0.12180 0.18346 The epi.directadj function provides additional outputs including the crude, crude by stratum and adjusted values along with the 95% confidence intervals. The adjusted value for population 1 = 0.25 and the adjusted value for population 1 = 0.15 so the relative risk is 1.66667 as calculated previously for the older standard population. It is most helpful to include a measure of the variation around this point estimate and the general structure is: Point estimate ± [1.96 × SE (estimate)] where 1.96 is the z-score (standard normal score) corresponding to the 95% confidence level (i.e., an alpha error of 5%). Confidence intervals for different levels of alpha error obtained by replacing this value with the corresponding z-score value (e.g., 1.64 for 90%, 2.58 for 99% confidence intervals). Because the RR is a multiplicative measure and thus asymmetrically distributed, its SE needs to be calculated in a logarithmic scale. \\[Se(log RR) = \\sqrt{\\frac{b}{a + b} + \\frac{d}{c + d}}\\] where a = exposed cases, b = exposed non cases, c = unexposed cases, d = unexposed non cases The 95% CI calculated on the logarithmic scale is therefore log(95%CI RR) = log RR ± 1.96 X SE(log RR) log_se &lt;- sqrt((150/(150*450)) + (90/(90+510))) log_CI &lt;- log(rr) + c(-1,1)* 1.96 * log_se CI &lt;- exp(log_CI) The 95% CI for the relative risk are therefore from 0.77578 to 3.58061. Remembering that these CIs are calculated on a multiplicative scale it is noticed that the point estimate is not the arithmetic mean of the confidence limits. Rather the point estimate equals the geometric mean, so as expected \\(\\sqrt{.78 * 3.58} = 1.66\\). This follows from lower limit 95% CI (RR) = RR × e–[1.96 × SE(log RR)] and upper limit 95% CI (RR) = RR × e[1.96 × SE(log RR)] so lower CI * upper CI = RR2 or RR = \\(\\sqrt{LCI * UCI}\\). 3.5.2 Example 5 - Contingency Tables with epiR package A most useful function for contingency tables is epi.2by2() from the epiR package which calculates multiple measures of association. But it needs a specific 2 by 2 table that must look like this for risk and odds ratio Outcome + Outcome - Exposure + Exposure - or like this for an incidence rate ratio. No. events PersonTime Exposure + Exposure - epi.2by2 is a useful function but its first argument must be a table object and the cell order must be exactly as shown above. Using the heart.csv data, what is the risk ratio for women with men as the reference (non-exposed) group? What is the odds ratio for the same association? # Remember epi.2by2 needs the table in the right order # 1st column is events and the 2nd row is the reference group # previous tab needs to be adjusted accordingly # risk ratio tab&lt;- tab[c(2,1),c(2,1)] epi.2by2(tab, method=&quot;cohort.count&quot;) ## Outcome + Outcome - Total Inc risk * Odds ## Exposed + 104 96 200 52 1.083 ## Exposed - 111 189 300 37 0.587 ## Total 215 285 500 43 0.754 ## ## Point estimates and 95% CIs: ## ------------------------------------------------------------------- ## Inc risk ratio 1.41 (1.15, 1.71) ## Odds ratio 1.84 (1.28, 2.65) ## Attrib risk * 15.00 (6.18, 23.82) ## Attrib risk in population * 6.00 (-0.98, 12.98) ## Attrib fraction in exposed (%) 28.85 (13.19, 41.68) ## Attrib fraction in population (%) 13.95 (5.16, 21.93) ## ------------------------------------------------------------------- ## Test that OR = 1: chi2(1) = 11.016 Pr&gt;chi2 = &lt;0.001 ## Wald confidence limits ## CI: confidence interval ## * Outcomes per 100 population units # odds ratio epi.2by2(tab, method=&quot;case.control&quot;) ## Outcome + Outcome - Total Prevalence * Odds ## Exposed + 104 96 200 52 1.083 ## Exposed - 111 189 300 37 0.587 ## Total 215 285 500 43 0.754 ## ## Point estimates and 95% CIs: ## ------------------------------------------------------------------- ## Odds ratio (W) 1.84 (1.28, 2.65) ## Attrib prevalence * 15.00 (6.18, 23.82) ## Attrib prevalence in population * 6.00 (-0.98, 12.98) ## Attrib fraction (est) in exposed (%) 45.72 (20.74, 62.91) ## Attrib fraction (est) in population (%) 22.15 (9.22, 33.23) ## ------------------------------------------------------------------- ## Test that OR = 1: chi2(1) = 11.016 Pr&gt;chi2 = &lt;0.001 ## Wald confidence limits ## CI: confidence interval ## * Outcomes per 100 population units This risk (or odds) ratio) can hand checked by calculating the ratio of the risk (or odds) for women and men. cat(&quot;Hand calculated risk ratio is &quot;, (tab[1,1] /(tab[1,2] + tab[1,1])) / (tab[2,1] /(tab[2,2] + tab[2,1]))) ## Hand calculated risk ratio is 1.4054 cat(&quot;\\nHand calculated odds ratio is &quot;, (tab[1,1] /tab[1,2]) / (tab[2,1] /tab[2,2])) ## ## Hand calculated odds ratio is 1.8446 What is the incidence rate ratio for women with men as the reference group? Hint: need to calculate the total number of events and person time according to gender. # Base R - tapply for SUM of people who DIED by GENDER and the SUM of FOLLOW-UP TIME by GENDER events &lt;- tapply(heart$fstat==&quot;dead&quot;, heart$gender, sum) persontime &lt;- tapply(heart$lenfol, heart$gender, sum) # make a 2 by 2 table, remembering to make the unexposed group the 2nd row tab_gender &lt;- cbind(events, persontime) tab_gender &lt;- tab_gender[c(2,1),] epi.2by2(tab_gender, method = &quot;cohort.time&quot;) ## Outcome + Time at risk Inc rate * ## Exposed + 104 165492 0.0628 ## Exposed - 111 275726 0.0403 ## Total 215 441218 0.0487 ## ## Point estimates and 95% CIs: ## ------------------------------------------------------------------- ## Inc rate ratio 1.56 (1.18, 2.06) ## Attrib rate * 0.02 (0.01, 0.04) ## Attrib rate in population * 0.01 (-0.00, 0.02) ## Attrib fraction in exposed (%) 35.94 (15.47, 51.42) ## Attrib fraction in population (%) 17.38 (12.96, 21.95) ## ------------------------------------------------------------------- ## Wald confidence limits ## CI: confidence interval ## * Outcomes per 100 units of population time at risk Of course, this can also be accomplished with the tidyverse approach using the dplyr package. # Tidyverse approach heart_rate &lt;- heart %&gt;% group_by(gender) %&gt;% summarise(events = sum(fstat==&quot;dead&quot;),time = sum(lenfol)) %&gt;% select(-1) %&gt;% as.matrix heart_rate &lt;- heart_rate[c(2,1),] epi.2by2(heart_rate, method = &quot;cohort.time&quot;) ## Outcome + Time at risk Inc rate * ## Exposed + 104 165492 0.0628 ## Exposed - 111 275726 0.0403 ## Total 215 441218 0.0487 ## ## Point estimates and 95% CIs: ## ------------------------------------------------------------------- ## Inc rate ratio 1.56 (1.18, 2.06) ## Attrib rate * 0.02 (0.01, 0.04) ## Attrib rate in population * 0.01 (-0.00, 0.02) ## Attrib fraction in exposed (%) 35.94 (15.47, 51.42) ## Attrib fraction in population (%) 17.38 (12.96, 21.95) ## ------------------------------------------------------------------- ## Wald confidence limits ## CI: confidence interval ## * Outcomes per 100 units of population time at risk 3.5.3 Example 6 - Contingency Tables with other R packages An advantage of r is that there are often multiple ways to get the right answer to a problem. Similarly, there are multiple packages that can used used to calculate epidemiologic measures of association. With the heart.csv dataset, what is the incidence rate ratio for women with men as the reference group? a) Using epi2X2() in the epibasix package. b) Using epitab() in the epitools package. Again be careful that the data is entered in the correct format of rows and columns required by each package. The required structure varies between packages and can be checked with help(package=\"XXX\"). Using the wrong structure would produce incorrect results. # Using `epi2X2()` in the `epibasix` package #cases should be entered as column one and controls as column two. #treatment as columns and outcome as row -&gt; need to transpose the original 2X2 table summary(epi2x2(t(tab))) # note need to transpose the rows ## Epidemiological 2x2 Table Analysis ## ## Input Matrix: ## ## women men ## dead 104 111 ## alive 96 189 ## ## Pearson Chi-Squared Statistic (Includes Yates&#39; Continuity Correction): NA ## Associated p.value for H0: There is no association between exposure and outcome vs. HA: There is an association : NA ## p.value using Fisher&#39;s Exact Test (1 DF) : 0.001 ## ## Estimate of Odds Ratio: 1.845 ## 95% Confidence Limits for true Odds Ratio are: [1.283, 2.652] ## ## Estimate of Relative Risk (Cohort, Col1): 1.436 ## 95% Confidence Limits for true Relative Risk are: [1.16, 1.778] ## ## Estimate of Risk Difference (p1 - p2) in Cohort Studies: 0.147 ## 95% Confidence Limits for Risk Difference: [0.056, 0.237] ## ## Estimate of Risk Difference (p1 - p2) in Case Control Studies: 0.15 ## 95% Confidence Limits for Risk Difference: [0.059, 0.241] ## ## Note: Above Confidence Intervals employ a continuity correction. # Using `epitab()` in the `epitools` package. #rows and columns need to be reversed/transposed epitab(tab, method = &quot;riskratio&quot;, rev = &quot;both&quot;) ## $tab ## ## alive p0 dead p1 riskratio lower upper p.value ## men 189 0.63 111 0.37 1.0000 NA NA NA ## women 96 0.48 104 0.52 1.4054 1.152 1.7146 0.0012243 ## ## $measure ## [1] &quot;wald&quot; ## ## $conf.level ## [1] 0.95 ## ## $pvalue ## [1] &quot;fisher.exact&quot; epitab(tab, method = &quot;oddsratio&quot;, rev = &quot;both&quot;) ## $tab ## ## alive p0 dead p1 oddsratio lower upper p.value ## men 189 0.66316 111 0.51628 1.0000 NA NA NA ## women 96 0.33684 104 0.48372 1.8446 1.2829 2.6523 0.0012243 ## ## $measure ## [1] &quot;wald&quot; ## ## $conf.level ## [1] 0.95 ## ## $pvalue ## [1] &quot;fisher.exact&quot; Thus it can be seen that the risk ratio and odds ratios as calculated with 3 different functions from 3 different R packages produce the same results. "],["infer.html", "Chapter 4 Statistical inference 4.1 Introduction 4.2 Null Hypothesis Significance Testing 4.3 S values 4.4 Two views of probability 4.5 Bayesian reasoning to understand “Why Most Published Research Findings are False” 4.6 Bayesian data analysis and statistical inference", " Chapter 4 Statistical inference 4.0.1 R packages required for this chapter library(knitr) library(tidyverse) library(purrr) library(epiR) library(forestplot) 4.1 Introduction It should be remembered that quality study design and data collection are required before any meaningful statistical inference can take place. In other words, there are plenty of places for a study to go wrong before statistical inference is performed and these design topics will be discussed in later chapters. For example, it is important to establish if - the sample representative of the population that we’d like to draw inferences about? - there systematic bias created by selection, misclassification or missing data at the design or during conduct of the study? - there are known and observed, known and unobserved or unknown and unobserved variables that contaminate our conclusions? Adoption of many research findings, by individual researchers, clinicians, public health experts, professional societies, guideline writers and regulatory agencies, are often unequivocally determined by a deification of p &lt; 0.05. While the many limitations and large potential for misinterpretations with this approach have long been appreciated in the statistical literature, these issues are considered somewhat esoteric and of lesser importance in the clinical literature. Common and important statistical inference misconceptions Biological understanding and previous research have little formal role in the interpretation of quantitative results. Statistical methods alone can provide a number that by itself reflects a probability of reaching true / erroneous conclusions Standard statistical approach implies that conclusions can be produced with certain “random error rates,” without consideration of internal biases and external information p values and hypothesis tests, are a mathematically coherent approach to inference Statistical inference is the process of generating conclusions about a population from a sample, without it we’re left simply within our data. As samples are inherently noisy, this is an essential process in going from data -&gt; knowledge. Probability models connect sample data and populations and are therefore an essential element of statistical inference. In probability theory, the model is known, but the data are not. In statistics we have the data, not the data generating model and want to learn the underlying “truth.” Example - The wrong probability model leading to the wrong inference Consider this well known and tragic case of a British mother and lawyer who had 2 children die of sudden infant death syndrome (SIDS). From this data we would like to infer whether or not this occurence is beyond the play of chance or whether Ms. Clark should be tried for infanticide. An expert testified that the probability of SIDS = 1 / 8,500 and therefore the probability of 2 deaths in 1 family was (1 / 8,500)\\(^2\\) or 1 in 72 million. The inference from this argument is that the probability of this occurring by chance, or “naturally,” was vanishingly small. The mother was convicted, undoubtedly in part based on this expert testimony. Do you agree with this inference? The expert has assumed an underlying probability model that views these 2 deaths as being independent and identical distributed (iid). But these 2 SIDS deaths are not independent events, rather they have a strong family occurrence, possibly related to both genetic and common environmental factors. In fact, the risk of a 2nd SDIS death in the same family is not 1 in 8500 but about 1 in 300. There are about 700,000 annual UK births and therefore approximately 82 first SIDS deaths. If SIDS families decide to have a 2nd child then it is expected that a 2nd death will occur in a little less than 4 years (4 * 82 = 320 2nd births in a SIDS family). This more “informed” probability model predicts a 2nd SIDS death in the same family far more frequently than that implied by a 1 in 72 million chance. It is impossible to say exactly how much damage the choice of the original probability model and the subsequent incorrect inference made to the conviction and the ensuing unfortunate circumstances (prison and eventual maternal death) but it seems likely to have been far from trivial. In addition to discussion of the advantages and limitations of the various statistical inference paradigms, this chapter will provide several examples illustrating how even landmark trials in prestigious journals can present statistical problems that substantially undermine their conclusions or even make them unreliable. The goal of this chapter is to encourage a more critical assessment and appraisal of the underpinnings of statistical inference in contemporary medical research. 4.2 Null Hypothesis Significance Testing The standard scientific approach to statistical inference depends on the two key components of p values and null hypothesis significance testing (NHST). Despite well recognized limitations(Amrhein, Greenland, and McShane 2019)(Wasserstein, Schirm, and Lazar 2019), NHST was an important advancement in scientific reasoning as it attempted to codify scientific decision making and elevated it above the historical subjective decision-making process. The NHST framework is summarized in Table 1. NHST framework presented in a 2 X 2 table The archetypal NHST roadmap is to set the α or type I (false positive) error rate typically to 5% and the β or type II (false negative) error rate 20% at the design level. While this approach will limit the number of errors made in the long run, hence the term “frequentist” methods, it fails to provide an inferential measure of the evidence for a given experiment. This problem has seen the development of an ersatz solution, the (in)famous p value(S. N. Goodman 1999a). The p value is the probability of getting a result (specifically, a test statistic) at least as extreme as what was observed if every model assumption, in addition to the targeted test hypothesis (usually a null hypothesis), used to compute it were correct. As test statistics are composed of an effect size divided by a standard error, a small p value may occur when there is a trivial effect size but a very large sample size leading to a small standard errors. Given that these characteristics don’t reflect what most consumers of medical research want, it is perhaps not surprising that misinterpretations of the p value abound. Common and egregious misinterpretations of p values The p-value is the probability that the null hypothesis is true The p-value is the probability that a finding was due to chance The p-value is the probability of falsely ejecting the null hypothesis The significance level, such as 0.05, is determined by the p-value The p-value indicates the size or importance of the observed effect While these misinterpretations are increasingly acknowledged and the use of confidence intervals encouraged, it is important to understand that confidence intervals are not a complete panacea. It is easiest to begin with what the CI is not telling us. It does not mean there is a 95% probability that the true effect size lies in this interval. While that is what we might like the 95% CI to provide, such an interpretation would require a Bayesian mindset that considers prior probabilities and assumes that the true effect is not a fixed but unknown quantity but rather a random variable. Moreover, the problems with p values are not limited to misinterpretations of its definition. Interpretative issues from the combined p values and NHST paradigm Misunderstanding that p value can simultaneous measure the evidential meaning of a single result and the long-run outcomes of a repeated experiment Favors dichotomized / binary thinking (statistically significant or not) with a loss of information Attaches excessive weight to the null hypothesis Discourages estimation of effect size and its uncertainty such that statistical and clinical significance are often confused (e.g. trivial differences in the presence of large small sizes may lead to statistical significance) Conventional p values &lt; 0.05 don’t provide strong evidence to reject the null hypothesis (approximately equivalent to 4-5 successive heads from a fair coin) P &gt; 0.05 does not necessarily equate to no effect, possibly arising from inadequate power (“Absence of evidence is not evidence of absence”) Does not allow the incorporation of prior knowledge or evidence One reason for the popularity of the NHST paradigm, despite its these limitations, is that it appears to avoid the conundrum of inductive reasoning and apparently represents the deductive Popperian view of scientific learning through falsification. However, this is more illusory than real as typically the researcher believes the alternative hypothesis is true and then constructs a straw man null hypothesis which (s)he doesn’t believe and hopes it is proven false. The alternative hypothesis is then accepted without any further attempt at falsification. The reality therefore resembles more a confirmationist than falsificastionist approach(Gelman 2014). Other major NHST limitations are the excessive weight given to the null hypothesis, as known as nullism(Greenland 2017), the non-specific nature of the accepted alternative hypothesis and a forced dichotomized decisional process(Wasserstein and Lazar 2016). The institutionalized bias of nullism can result in a blunting of scientific discovery process, an endorsement of an artifical certainty of no effect and an aversion to the existence and uncertainty surrounding non-null hypotheses. While NHST describes a deductive behavior to limit mistakes for repeated experiments in the long run, its biggest limitation as mentioned above is a lack of an inductive measure of the evidence for a given experiment. The incorporation of the p value into the NHST paradigm attempts to address this shortcoming but the presentation of these two concepts don’t represent a unified theory, but actually are separate, and perhaps irreconcilable schools(S. N. Goodman 1999a). In short, p values can’t be expected to provide both a “short run” perspective, which is evidential and inductive and a long-run perspective, which is error-based and deductive. Example 1 - Incorrect p value interpretation - Don’t compare p values A 2011 study reported that selective COX-2 inhibitors (NSAIDs) were associated with atrial fibrillation (RR 1.20, 95% CI 1.09 - 1.33, p&lt;0.01) A later 2013 study concluded “use of selective COX-2 inhibitors was not significantly related to atrial fibrillation occurrence” (RR 1.20, 95% CI 0.97 - 1.47, p=.23). These authors went on to elaborate why the results were different - slightly different populations, etc. The fundamental question is \" Are the 2 results are really different?\" While one study reached statistical significance and the other didn’t, making statistical inference based on comparing P values from separate analyses is incorrect. The correct approach is discussed by Altman and Bland and involves a formal test for interaction. The calculations comparing relative risks or odds ratios are analyzed on the log scale because the distributions of the log ratios tend to be those closer to normal than of the ratios themselves. The comparison of these two summary statistics is well described in the Altman and Bland article and can be operationalized in R as follows: inter &lt;- function(RR, lci, uci){ #enter RR, lci, uci as vectors of length 2 RR &lt;- log(RR) lci &lt;- log(lci) uci &lt;- log(uci) width &lt;- abs(lci -uci) se &lt;- width / 3.92 delta &lt;- RR[1] - RR[2] se_delta &lt;- sqrt(se[1]^2 + se[2]^2) #variance of a difference = sum of individual variances ci_delta &lt;- delta + c(-1,1) * 1.96 * se_delta z &lt;- delta / se_delta pvalue &lt;- 2* pnorm(z) RRR &lt;- exp(delta) ci_RRR &lt;- exp(ci_delta) return (list(&quot;z value for interaction test&quot; = z, &quot; associated p value&quot; = pvalue, &quot;Relative risk ratio&quot; = RRR, &quot;95% CI&quot; = ci_RRR)) } inter(c(1.27,1.20), c(1.20,0.92), c(1.34,1.48)) ## $`z value for interaction test` ## [1] 0.4554 ## ## $` associated p value` ## [1] 1.351 ## ## $`Relative risk ratio` ## [1] 1.058 ## ## $`95% CI` ## [1] 0.8292 1.3508 From this analysis, it is obvious that there is no statistically significant difference between the 2 results and rather than trying to explain non-existent differences, the authors should have concluded that their results, while under powered, were entirely compatible with the previous results. Data visualization rather simply but elegantly and convincingly demonstrates the consistency of these two results, dat &lt;- matrix(c(1.27, 1.20, 1.34, 1.20, 0.92, 1.48), nrow = 2, byrow = TRUE, dimnames = list(c(&quot;2011 study&quot;,&quot;2013 study&quot;), c(&quot;coef&quot;, &quot;lower&quot;, &quot;upper&quot;))) clrs &lt;- fpColors(box = &quot;royalblue&quot;,line = &quot;darkblue&quot;, summary = &quot;royalblue&quot;) forestplot(rownames(dat), mean=dat[,1], lower=dat[,2], upper=dat[,3], col = clrs, grid = structure(c(1), gp = gpar(lty = 2, col = &quot;#CCCCFF&quot;)), xlab = &quot;RR&quot;) Example 2 - Incorrect p value interpretation - p &gt; 0.05 \\(\\neq\\) no effect In the STICH trial, a total of 1212 patients with an ejection fraction &lt; 35% and coronary artery disease amenable to coronary artery bypass surgery (CABG) were randomly assigned to medical therapy alone (602 patients) or medical therapy plus CABG (610 patients). The primary outcome was death from any cause. The primary outcome occurred in 244 patients (41%) in the medical-therapy group and 218 (36%) in the CABG group (hazard ratio with CABG, 0.86; 95% CI, 0.72 to 1.04; P = 0.12). The abstract conclusion stated “In this randomized trial, there was no significant difference between medical therapy alone and medical therapy plus CABG with respect to the primary end point of death from any cause.” With a p value &gt; 0.05, the authors have interpreted this as a “negative” study and concluded “no significant difference between medical therapy alone and medical therapy plus CABG.” However, this is only a negative study when viewed through the artificially dichotomized lens of significance testing. Although confidence intervals have been reported in addition to the p value, they have been interpreted in the same restricted dichotomized manner as p values, namely that inclusion of a HR = 1 in the confidence interval equates to a non-significant finding. In fact, these results actually support the opposite conclusion, that a difference between the two treatments exists, and it favors CABG! To help appreciate this new interpretation, remember the choice of p &lt; 0.05 as the definition of statistical significance is arbitrary and merely chosen on a tradition borrowed from agricultural research in the 1920’s. Different thresholds (p values) for statistical significance could be chosen and they will directly impact the width of the confidence intervals. This relationship can be shown graphically and this p value graph underscores that decision making is better appreciated as a continuous rather than binary process. For example, accepting an 83% CI provides a statistically significant interval since it excludes the null value of HR=1. pv_graph &lt;- function(hr, uci, lci) { se &lt;- (log(uci)-log(lci))/(2*1.65) x &lt;- seq(0.01, 0.50,by = .005) p1 &lt;- exp(log(hr) - (qnorm(x) * se)) p2 &lt;- exp(log(hr) + (qnorm(x) * se)) p &lt;- data.frame(x, p2, p1) g &lt;- ggplot(p, aes( p2, x)) + geom_line() + geom_line(aes(p1, x)) + scale_x_continuous(trans=&#39;log10&#39;) + ylab(&quot;p value \\n one sided&quot;) + xlab(&quot;Hazard ratio (Log scale)&quot;) + labs (title=&quot;P value function&quot;) + geom_hline(yintercept=c(.005,.025,0.05,0.10), color = &quot;red&quot;) + annotate(&quot;text&quot;, x=.7,y=.01, label=&quot;99% CI&quot;) + annotate(&quot;text&quot;, x=0.74,y=.04, label=&quot;95% CI&quot;) + annotate(&quot;text&quot;, x=0.78,y=.06, label=&quot;90% CI&quot;) + annotate(&quot;text&quot;, x=.82,y=.11, label=&quot;80% CI&quot;) + geom_vline(xintercept=1.0, color = &quot;green&quot;) + annotate(&quot;text&quot;, x=1,y=.4, label=&quot;null hypothesis&quot;) + theme_bw() return(g) } stich_pv_5 &lt;- pv_graph(0.86, 1.04, 0.72) + labs (title=&quot;Stich trial results 2011&quot;, subtitle = &quot;P value function for HR = 0.86, 95% CI 0.72 to 1.04&quot;) stich_pv_5 This figure shows that accepting an 83% CI provides a statistically significant interval since it excludes the null value of HR=1. Of course, this doesn’t supply conclusive evidence for the superiority of CABG but does show the presence of some weight in its favor in contrast to the typical interpretation of a dichotomized p value &gt; 0.05. A more nuanced conclusion would be that due to the limited statistical power, definitive evidence for the superiority of either technique is not available. However the best estimate is a 14% reduction in mortality with CABG, with sampling variability such that the benefit may be as large 28% or even a 4% increased risk. Perhaps this figure of a cultural icon will help avoiding this error. 4.3 S values Even when p values are correctly understood, their scaling can make an appreciation of their strength of evidence against the null hypothesis problematic. As seen in the figure below from (Rafi and Greenland 2020) the same absolute change in p value is associated with vastly different meanings for differences in pvalues near 1 compared to those near 0. Enhanced understanding of the strength of the evidence against not only the null hypothesis but against any specific alternative hypotheses can be more easily appreciated by considering p values not on their natural probability scale from 0 to 1 but on a scale that reflects the probability of successive tosses, S, of an unbiased coin showing only heads, p = (1/2)s. This is known as the binary Shannon information, surprisal or S value which can be rearranged as S = log2(1/p) = −log2(p) and is a measure of the evidence against the test (null) hypothesis. STICH example continued Returning to the STICH example, A graph of S versus the HR reveals that evidence against any hypothesis is minimized at the point estimate (S=0 at HR =0.86). s_graph &lt;- function(hr, uci, lci){ se &lt;- (log(uci)-log(lci))/(2*1.65) #.86 0.72 to 1.04 (log se) x &lt;- seq(0.01, 0.50,by = .005) lci &lt;- exp(log(hr) - (qnorm(x) * se)) uci &lt;- exp(log(hr) + (qnorm(x) * se)) lci &lt;- rev(lci) hr &lt;- rev(c(uci, lci)) yy &lt;- 2*x yy &lt;- c(yy,rev(yy)) ss &lt;- -log(yy, base=2) df1 &lt;- data.frame(hr,ss) df1 &lt;- df1[-297,] s &lt;- ggplot(df1, aes( hr,ss)) + geom_line() + xlim(0.01,1.2) + scale_x_continuous(trans=&#39;log10&#39;) + ylab(&quot;Bits of information against HR (binary S value)&quot;) + xlab(&quot;Hazard ratio (Log scale)&quot;) + labs (subtitle = &quot;S-Values (surprisals) for a range of hazard ratios (HR)&quot;) + geom_vline(xintercept=1.0, color = &quot;green&quot;) + annotate(&quot;text&quot;, x=1,y=.4, label=&quot;null hypothesis&quot;) + theme_bw() return(s) } stich_s_5 &lt;- s_graph(0.86, 1.04, 0.72) + labs(title=&quot;Stich trial results 2011&quot;) + annotate(&quot;text&quot;, x=.8,y=1, label=&quot;Maximum likelihood estimate (HR=0.86)\\n has the least refutational evidence \\n against it (0 bits)&quot;) + geom_segment(aes(x = .86, y = 0.8, xend = .86, yend = 0.015), arrow = arrow(length = unit(0.5, &quot;cm&quot;)),color=&quot;red&quot;) stich_s_5 There is also very little evidence against a 10% decrease with CABG. Surprisingly, there is even less evidence against the hypothesis of a 25% decrease with CABG than there is against the null hypothesis which we have been told we should accept! The graph also shows a clinically meaningful increased risk (arbitrarily defined as HR =1.15 with CABG is unlikely, equating to getting about 6 heads in a row from a fair coin. In fact, a putative 32% benefit has the same amount of refutational evidence against it as a 10% increased risk. This example speaks again to the observation that the difference between “significant” and “not significant” is not itself statistically significant(Gelman and Stern 2006). Examined from this perspective, despite the authors’ interpretation of the STICH study as negative, it would have been eminently reasonable, based on these results, to have offered offer CABG to eligible patients while awaiting the accumulation of additional evidence. The confirmation of this approach was provided by a 2016 STCIH publication which had increased power due to a longer follow-up and showed that the rates of death from any cause, death from cardiovascular causes, and death from any cause or hospitalization for cardiovascular causes were significantly lower over 10 years among patients who underwent CABG in addition to receiving medical therapy than among those who received medical therapy alone (HR 0.84; 95% CI 0.73 to 0.97). 4.4 Two views of probability Two views of probability In the NHST paradigm, since parameters considered as fixed but unknown quantities, we can’t make probability statements about them. In this context, probability is limited to sampling variability, i..e. in the long run proportion of times an event occurs in independent, identically distributed (iid) repetitions. Answers questions like “What should I decide given my data controlling the long run proportion of mistakes I make at a tolerable level.” In the Bayesian paradigm, parameters are random variables that follow the rules of probability. In this context, probability is a calculus of beliefs that answer questions like “Given my subjective beliefs and the objective information from the data, what should I believe now?” Basic Probability Example Using R Background: During an exam suspected that one student is copying answers from another student. There is a choice of 4 answers for each question. The Evidence: On the 16 questions missed by both students, 13 of the answers were the same. Question: Is this data beyond the play of chance? Did the student cheat? Answer: There is a 1 in 4 probability that the students will randomly choose the same answer. For 16 questions, one would expect 4 answers to be the same but there is obviously variability in this expected value. One could use chisq.test as in section 3.1 but with small sample using the binomial distribution describes more fully this scenario and help can be found with help(rbinom). # chi sq test x &lt;- c(13,3) chisq.test(x, p = c(.25,.75)) ## Warning in chisq.test(x, p = c(0.25, 0.75)): Chi-squared approximation may be ## incorrect ## ## Chi-squared test for given probabilities ## ## data: x ## X-squared = 27, df = 1, p-value = 2e-07 # to see the variability draw 10000 times from a sample of 16 with a probability = 1/4 df &lt;- rbinom(10000,16,.25)/16 #plot the histogram ggplot(as.data.frame(df), aes(df)) + geom_histogram(breaks=seq(0, .45, by = .006)) + xlim (c(.0,.85)) + ggtitle(&quot;Histogram of same responses by chance&quot;) + geom_vline(xintercept=.8125, color = &quot;green&quot;) + annotate(&quot;text&quot;, x=.8125,y=.50, label=&quot;observed value&quot;) + theme_bw() Based on probabilities,a proportion of 0.8125 agreement would be very unusual if second student was simply guessing, so we conclude that second student was not guessing. However, we can’t extrapolate this to conclude there was cheating. If you don’t know the answer to a question on an exam, you rarely guess completely at random. Hopefully you will make an educated guess &amp; some wrong answers might be more logical than others. This could explain the large proportion of matches on wrong answers between the two students. So this evidence is not convincing evidence that the student cheated, but we know that he did not just guess. 4.4.1 Basic rules of probability As the rules of probability are the basic building block for Bayesian inference, let’s quickly review them. By definition, probabilities must be between 0 and 1 If an event occurred then P = 1 If an event can’t occur then P = 0 P(A) + P(\\(\\overline{A}\\)) = 1 where P(\\(\\overline{A}\\)) is the compliment of P(A) If 2 events are mutually exclusive then p(A U B) = p(A) + p(B) U = union If 2 events are not mutually exclusive then P(A U B) = P(A) + P(B) - P(A ∩ B) n = intersection (addition rule) P(A ∩ B) = P(A|B) * p(B) (multiplication rule) P(A) = P(A n B) + P(A n \\(\\overline{B}\\)) (total probability rule) The addition, multiplication and total probability rules are easily understood with the use of Venn diagrams. Figure 4.1: Addition rule Figure 4.2: Multiplication rule Figure 4.3: Total probability rule 4.4.2 Bayes Theorem From these rules of probability, it is easy to develop the basic form of Bayes Theorem. 1st application of multiplication rule: \\[\\begin{equation} P(A ∩ B) = P(A|B)*P(B) \\tag{4.1} \\end{equation}\\] 2nd application of multiplication rule: \\[\\begin{equation} P(B ∩ A) = P(B|A)*P(A) \\tag{4.2} \\end{equation}\\] From (4.1) and (4.2): \\[\\begin{equation} P(A|B)*P(B) = P(B|A)*P(A) \\tag{4.3} \\end{equation}\\] or \\[\\begin{equation} P(A|B) = \\frac{P(B|A)*P(A)}{P(B)} \\tag{4.4} \\end{equation}\\] which is Bayes Theorem where occasionally the denominator is expanded using the rule of total probability \\[\\begin{equation} P(A|B) = \\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})} \\tag{4.5} \\end{equation}\\] Example 1 Bayes Theorem This example comes from the excellent book by Daniel Kahneman “Thinking fast, thinking slow”(Kahneman 2011) which explores cognitive biases. Steve is described by his neighbor as a shy individual, very helpful but he has little interest in people. He likes things in their proper order, and is very detailed about his work. Do you think Steve is more likely to be a librarian or a farmer? It may seem that Steve is more likely to be a librarian and most people would agree with this conclusion but that’s ignoring the background distribution of librarians and farmers. Bayes theorem with its form of updating a prior probability intrinsically avoids this error of ignoring the baseline context or probability. Before considering Steve’s characteristics, the odds of male farmers to male librarians may be assumed to be in the order of 20 to 1. So based on this information, Steve is statistically more likely to be a farmer. Let us explore this is in more detail with Bayes Theorem. Let A = probability that Steve is a librarian. Recall odds = P(event) / P(\\(\\overline{event}\\)) or probability = odds / (odds +1) so P(A) = 1/21 and P(\\(\\overline{A}\\)) = 20/21 Let B = Steve’s characteristics What is P(B |A)? This is the probability that the neighbor would describe Steve in these terms if he was indeed a librarian. This is unknown but is presumably close to 1. Let’s assume 0.95. Looking at equation (4.5), the only term missing on the left of the equation is P(B |\\(\\overline{A}\\)), the probability that Steve would have these characteristics if he was not a librarian. Again this is unknown but presumably this would be less than P(B |A), let’s assume 0.3. We may now substitute into (4.5) and calculate our updated probability that Steve is a librarian given his characteristics. P(A | B) = \\(\\frac{(0.95) * (1 /21)}{(0.95) * (1 /21) + (0.3) * (20/21)}\\) = 0.136 Bayes Theorem has obliged us to consider the importance of the baseline rate, or context, which in this case is that farmers are a much more common vocation among males. However Steve’s characteristics have shifted our probability that he is a librarian from 4.8% to 13.6% but it is still much more likely that he is a farmer. This is at the essence of diagnostic testing where a positive test is unlikely to mean that disease is present if the baseline disease prevalence is low in the population being studied. Hence why diagnostic testing may be sometimes be inappropriate. Example 2 Bayes Theorem and Diagnostic Testing 25 year old MSc epi student got a tattoo on spring vacation in Cancun. Asymptomatic but is screened for Hepatitis C for an insurance policy. Test positive (sensitivity = 95%, specificity = 98%). Assuming the prevalence of HepC in this population is 0.1%. What is the probability (s)he truly has HepC? This problem can often be most easily understood by completing a 2X2 table as follow Disease + Disease - Test + 95 1998 2093 Test - 5 97902 97907 100 99900 100000 and then calculating the positive predictive value, P( D | T), calculated from the horizontal e first line of the above 2X2 table, = 95 / 2093 = 0.045. Alternatively one could directly apply Bayes rule as expressed in equation (4.5), \\[\\begin{equation} P(D|T) = \\frac{P(T|D)*P(D)}{P(T|D)\\*P(D)+P(T|\\overline{D})*P(\\overline{D})} = \\frac{.95*.001}{.95*.001 + .02*.999} = 0.045 \\end{equation}\\] It can be appreciated that even though the test has good sensitivity and specificity, the very low prevalence means that a positive test is most likely a false positive as the probability of a having disease given the positive test is only 4.5%. It is key to remember the distinction between test sensitivity (P(T|D)) and positive predictive value (P(D|T)) and that P(T|D) ≠ P(D|T). The confusion between these two probabilities is the source of a common bias known as the prosecutor’s fallacy in which the probability of the evidence given innocence is confused with the probability of innocence given the evidence (P(E|I) ≠ P(I|E)) In an analogous manner, it may be appreciated that the p value = P(Data | Ho is true) does NOT provide what most clinicians are looking for which is instead P(Ho is true | Data). 4.5 Bayesian reasoning to understand “Why Most Published Research Findings are False” John Ioannidis is one of the world’s most cited authors (&gt;325,000 citations) and his 2005 paper “[Why Most Published Research Findings are False]” (https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124&amp;xid=17259,15700019,15700186,15700190,15700248) has been cited almost 10,000 times and viewed online over 3 million times. The interpretation of the number of false conclusions in medical literature can be seen as analogous to the examination of diagnostic tests and this approach will be used to understand how Ioannidis reached his conclusions about the lack of veracity of many published research papers. Figure 1.2: Analogy between diagnostic testing and research findings Based on the diagnostic example above, it can be appreciated that a key determinant of the number of false findings will depend greatly on our beliefs concerning the prevalence of positive studies. It is only a minority of studies which will have positive results and this undoubtedly varies function of the research design. Even for the highest quality pre-selected studies, phase III RCTs, there is not more than a 50:50 chance that a study will ultimately yield a positive results. With other designs, the ratio of positive to negative studies may be considerably lower. The number of true positive studies will be a function not only of the baseline prevalence positivity, but also power (sensitivity) and significance level (specificity) characteristics, along with possible study biases. Figure 4.4: Positive predictive values for various research designs and parameter In Figure 4.4, Ioannidis proposes some relationships between the odds (R) of a true (alternative hypothesis) to not true relationship according to study design, bias (u) and power (1 - \\(\\beta\\)) where PPV = positive predictive value. Before examining the general formula, let’s numerically reproduce one of the PPVs, for example consider the adequately powered exploratory epidemiology study. Imagine there are 1,000 such studies and R = 1 / 10, therefore the probability of a positive study = R / (R+1) = 91 and 73 will discovered (80% power). The probability of a negaitve study = 1 /(R +1) and in this case there are 909 negative studies but 45 (5%) will be false positive. This is shown in the following figure and the PPV = 73 / 128. Figure 1.3: Adequately powered exploratory epidemiology study no bias Now imagine, as in most studies there is a bias and assume as in Figure 4.4 that is 30%. Bias most often leads to increased positive study results. This will result in a shift of 30% of studies from cells C and D to A and B, respectively as shown in the following figure. Figure 4.5: Adequately powered exploratory epidemiology study no bias The PPV is now = 78 /382 = 0.20 as in Figure 4.4. from Ioannidis’ original publication. Ioannidis gives the general formula to calculate PPV as a function of R, power, \\(\\alpha\\) and bias as follows Figure 4.6: General Assocation of Research Findings and True Relationships in the Presence of Bias While some of the assumptions and corollaries that Ioannidis has made have been the subject of criticisms (S. Goodman and Greenland 2007), the overarching usefulness of his approach and its basic premise built on Bayesian reasoning is, in my opinion, both sound and useful. Here is a simple R function that will calculate the positive predictive value for a positive study finding given a particular research design. Here we calculate the PPV for the above epidemiologic study with 30% bias as above. study_ppv &lt;- function(R, u, beta, alpha=0.05, c=1000){ # R = odds of true relationship # u = bias # beta = type II error # alpha = type II error # enter data as decimals pD &lt;- R/(R+1) p_noD &lt;- 1/(R+1) A &lt;- (c*(1 - beta)*R + u*c*beta*R)/(R + 1) C &lt;- c(1 - u)*c*beta*R/(R + 1) B &lt;- (c*alpha + u*c*(1 - alpha))/(R + 1) D &lt;- (1 - u)*c*(1 - alpha)/(R + 1) ppv &lt;- A / (A + B) t &lt;- paste(&quot;Based on Ioannidis&#39; 2005 paper, the positive predicted value of a true relationship is&quot;, round(ppv,3)) t1 &lt;- matrix(round(c(A,B,C,D),0), nrow = 2, byrow = TRUE) list(Result = t, Table=t1) } study_ppv(.1,.3,.2) ## $Result ## [1] &quot;Based on Ioannidis&#39; 2005 paper, the positive predicted value of a true relationship is 0.204&quot; ## ## $Table ## [,1] [,2] ## [1,] 78 305 ## [2,] 13 605 Can also reproduce the accompanying plot alpha &lt;- 0.05 beta &lt;- 0.2 c &lt;- 1000 u &lt;- 0.3 R &lt;- seq (0,1, length.out = 21) A &lt;- (c*(1 - beta)*R + u*c*beta*R)/(R + 1) B &lt;- (c*alpha + u*c*(1 - alpha))/(R + 1) ppv &lt;- A / (A + B) df &lt;- data.frame(x=R, y=ppv) df %&gt;% ggplot(aes(x,y)) + geom_line() + geom_segment(aes(x = .1 , y = 0, xend = .1, yend = .204)) + labs(x=&quot;Pre study odds&quot;, y=&quot;Post study probability (PPV)&quot;) + ggtitle(&quot;Probability Research Finding Is True as a Function of the Pre-Study Odds&quot;, subtitle = &quot;(Bias =0.2)&quot;) + theme_bw() Finally, we can also reproduce Ioannidis’ Figure 1a which displays post study probability as a function of pre study odds and the degree of bias. t &lt;- rerun(4, df) %&gt;% map_df( ~ tibble(.), .id = &quot;dist&quot;, x.x =&quot;x&quot;) %&gt;% mutate(bias = ifelse(dist == &quot;1&quot;, .05, ifelse(dist == &quot;2&quot;, .2, ifelse(dist == &quot;3&quot;, .5, ifelse(dist == &quot;4&quot;, .8, &quot;NA&quot;))))) %&gt;% mutate(alpha=0.05, beta=0.2, c=1000, bias=as.numeric(bias)) %&gt;% mutate(A = (c*(1 - beta)*x + bias*c*beta*x)/(x + 1)) %&gt;% mutate(B = (c*alpha + bias*c*(1 - alpha))/(x + 1)) %&gt;% mutate(ppv = A / (A + B)) ggplot(t, aes(x,ppv,color = as.factor(bias))) + geom_line() + labs(x=&quot;Pre study odds&quot;, y=&quot;Post study probability (PPV)&quot;) + labs(color=&#39;Level of bias&#39;) + labs(caption = &quot;power = 80%&quot;) + ggtitle(&quot;Probability Research Finding Is True as a Function of the Pre-Study Odds&quot;, subtitle = &quot;Varying levels of bias (alpha = 0.05, beta = 0.2)&quot;) + theme_bw() 4.6 Bayesian data analysis and statistical inference Unlike the standard frequentist approach described above, Bayesian inference is not based on statistical significance testing, where effects are compared to a “null hypothesis.” Standard (frequentist) procedures fix the working hypotheses and, by deduction, make inference on the observed data:\\ – If my hypothesis is true, what is the probability of randomly selecting the data that I actually observed? If small, then deduce weak support of the evidence to the hypothesis – Assess Pr(Observed data | Hypothesis) On the contrary, the Bayesian philosophy proceeds fixing the value of the observed data and, by induction, makes inference on the unobservable hypotheses, which is ultimately what both clinicians and researchers are most interested in. - What is the probability of my hypothesis, given the data I observed? If less than the probability of other competing hypotheses, then weak support of the evidence for the hypothesis - Assess Pr(Hypothesis | Observed data) Advantage of inductive reasoning: conclusions about unobserved states of nature are broader than the observations on which they are based; using this reasoning will generate new hypotheses and learning new things. The drawback is cannot be sure that what we conclude is actually true, a conundrum known as the problem of induction. Bayesian reasoning combines a belief calculus (prior) and an evidential calculus (likelihood ratio) where each individual is entitled to their own, subjective evaluation. According to the evidence that is sequentially available, individuals tend to update their beliefs. In identifiable problems as more data accumulates the subjective component diminishes and divergent opinions converge. In non-identifiable problems (missing data, measurement error, unmeasured confounders) priors remain important even as more data accumulates. There are two essential defining characteristics of the Bayesian paradigm. First, the Bayesian framework treats unknown parameters as random variables, thereby offering a probabilistic view of their assessment and of the associated uncertainty. The other crucial aspect of the Bayesian paradigm is the formalism of past knowledge that is incorporated with current data via Bayes Theorem, thereby providing a coherent approach to updating scientific knowledge that mirrors our normal sequential learning process. Figure 4.7: Bayes theorem This prime advantage of Bayesian reasoning is summarized by the following pithy aphorism “Today’s posterior is tomorrow’s prior” which reflects the sequential learning process of the method. Typically, the integration of the study data, known as the likelihood function, over the prior distribution space requires the use of numerical methods, most often Monte Carlo Markov Chain techniques. However in special situations, such as when both the prior and the likelihood are assumed to be normally distributed, closed form analytical solutions, known as Bayesian normal conjugate analyses are possible. In a closed-form analysis, the final (posterior) mean is simply a weighted average of both the prior mean and the sample mean with the weights being directly proportional to the respective precisions (inverse variance).(Gelman et al. 2014) The flexibility of Bayesian analyses permits a straightforward calculation of the posterior probability of the measured effect size exceeding any given threshold by a calculation of the area under the curve (AUC) to the right of the selected threshold. The visualization of these posterior probability functions greatly assists data interpretation. Other advantages of a Bayesian approach are greater flexibility in modeling complex processes, including hierarchical modeling, handling of missing data, and evaluating and comparing competing models. From the perspective of randomized clinical trials (RCTs), these advantages may translate into concrete benefits at the design, monitoring, analytical and interpretative phases. Concretely, these benefits may include possible sample size reduction via the inclusion of prior information and possible adaptive trial designs, enhanced monitoring for possible early terminations, direct user-friendly probability statements probability statements about predictive probabilities and a decision theory environment that incorporates both benefits and loss functions for enhanced clinical decision making. Against these benefits, several limitations of the Bayesian method have been raised. Initially, the evaluation of these high-dimensional posterior distributions was previously difficult and involved approximations and numerical methods but more recently the process has been simplified by high speed simulation methods allowing a wide expansion of its clinical applicability. The remaining major objection to the Bayesian approach is its reputed subjectivity due its incorporation of prior information. In fact, this is not always a bona fide deterrent. As priors must be transparently presented and are often based on pre-existing objective research findings, completely inappropriate and arbitrary priors are easily identified and excluded from consideration. Moreover, a range of priors can be tested that will permit an assessment of the robustness of the final (posterior) inferences. By comparison, there are multiple assumptions in frequentist analyses which are neither transparent nor properly tested including an assumption of the data generating process that in theory needs to be infinitely replicated to allow p-values and confidence limits to be computed. A rather unflattering comparison of the frequentist and Bayesian paradigms and assumptions has been proposed and is summarized below. Comparisons of underlying assumptions • Frequentist = subjectivity1 + subjectivity2 + objectivity + data + endless arguments about everything • Bayesian = subjectivity1 + subjectivity3 + objectivity + data + endless arguments about one thing (the prior) where • subjectivity1 = choice of the data model • subjectivity2 = sample space and how repetitions of the experiment are envisioned, choice of the stopping rule, 1-tailed vs. 2-tailed tests, multiplicity adjustments, … • subjectivity3 = prior distribution 4.6.1 Bayes factors As shown with the S values above, the evidential value of p values is often overestimated and may be further compromised in the presence of large sample sizes as small p values can be observed with trivial clinical differences. Recall that there are two components to the Bayesian paradigm, on the left of Figure 4.7 is an evidential calculus based on the likelihood ratio and on the right of Figure 4.7, the belief calculus that incorporates our prior beliefs. Concentrating initially uniquely on the evidential arm, the likelihood ratio, also known as the Bayes factor, provides a probabilistically justified measure of the strength of the evidence in support of competing hypotheses. \\[\\begin{equation} Bayes Factors = \\frac{Prob(Data | null hypothesis)}{Prob(Data | alternative hypothesis)} \\tag{4.6} \\end{equation}\\] Equation (4.6) demonstrates that Bayes factors 1) are ratios, and not individual probabilities, of comparative evidential support for the observed data 2) provide varying degrees of support for every possible hypothesis and 3) provide maximum support (minimal Bayes factor) for the hypothesis compatible the observed data. Unlike p values, Bayes factors are therefore not simply measures against a specific null hypothesis but also measure support for a competing hypothesis. Also, in contrast to p values, Bayes factors depend only on the observed data and not on long-term unobserved data. It has been shown[(S. N. Goodman 1999b) that the conversion rate between a p value and the minimum Bayes factor, where the denominator of Equation (4.6) is Prob(Data | best supporting hypothesis), is given as \\[\\begin{equation} Minimum Bayes Factor = \\exp^{\\frac{-Z^2}{2}} \\tag{4.7} \\end{equation}\\] where z, the number of standardized errors from the null. Qualitative correlations between Bayes factors and the strength of the evidence have been proposed and one such scheme is presented in the following table Bayes Factor Label &gt; 100 Extremely strong evidence for Ho 30 - 100 Very strong evidence for Ho 10 - 30 Strong evidence for Ho 3 - 10 Moderate evidence for Ho 1 -3 Anecdotal evidence for Ho 1 No evidence 1/3 - 1 Anecdotal evidence for Ha 1/3 - 1/10 Moderate evidence for Ha 1/10 - 1/30 Strong evidence for Ha 1/30 - 1/100 Very strong evidence for Ha &lt;1/100 Extremely strong evidence for Ha Bayes factors are not merely a recalibration of p values but also allows the extension of statistical results to inductive inferences. Given that \\(posterior odds = bayes factor * prior odds\\), if we have a Bayes factor equal to 1/10, it means that these study results have decreased the relative odds of the null hypothesis by 10-fold. For example, if the initial odds of the null were 1 (ie, a probability of 50%), then the odds after the study would be 1/10 (a probability of 9%). Bayes factors - simple example Suppose the prior probability of a given hypothesis is 50%, estimated from previous studies, theoretical considerations or simply experet opinion and an experimetn testing that hypothesis generates a p value = 0.05. What is the new (posterior) probability of the hypothesis? # create simple function to calculate the minimum BF min_bf &lt;- function(z){ bf &lt;- exp((-z^2)/2) paste(&quot;Minimum Bayes Factor = &quot;, round(bf,2), &quot;so there is &quot;, round(1/bf,2), &quot;times more evidence supporting the alternative hypothesis of the observed data than for the null of no benefit&quot;) } # Z for p = 0.05 is 1.96 min_bf(1.96) ## [1] &quot;Minimum Bayes Factor = 0.15 so there is 6.83 times more evidence supporting the alternative hypothesis of the observed data than for the null of no benefit&quot; post_prob &lt;- function(prior,bf){ odds &lt;- prior/(1-prior) post_odds &lt;- odds * bf post_prob &lt;- post_odds / (1+ post_odds) paste(&quot;If Bayes Factor = &quot;, round(bf,2), &quot;and the prior probability = &quot;, round(100*prior,2), &quot;%, the posterior probability = &quot;,round(100*post_prob,0), &quot;%&quot;) } post_prob(.5, 0.15) ## [1] &quot;If Bayes Factor = 0.15 and the prior probability = 50 %, the posterior probability = 13 %&quot; Goodman (S. N. Goodman 1999b) has produced this table which connects prior and posterior probabilities and Bayes Factors. Figure 4.8: P values, BF and probabilities A graphical form of this table can be constructed. df &lt;- data.frame (prior_prob=seq(0,.99, length.out = 99), post = seq(0,1, length.out = 99)) t &lt;- rerun(4, df) %&gt;% map_df( ~ tibble(.), .id = &quot;dist&quot;, x.x =&quot;x&quot;) %&gt;% mutate(bf = ifelse(dist == &quot;1&quot;, 1/5, ifelse(dist == &quot;2&quot;, 1/10, ifelse(dist == &quot;3&quot;, 1/20, ifelse(dist == &quot;4&quot;, 1/100, &quot;NA&quot;))))) %&gt;% mutate(bf=as.numeric(bf), prior_odds = prior_prob/(1-prior_prob)) %&gt;% mutate(post_odds = bf * prior_odds) %&gt;% mutate(post_prob = post_odds / (1+ post_odds)) ggplot(t, aes(prior_prob,post_prob,color = as.factor(bf))) + geom_line() + labs(x=&quot;Prior probability Ho true&quot;, y=&quot;Posterior probability Ho true&quot;) + labs(color=&#39;Bayes factor&#39;) + geom_hline(yintercept = 0.05, color=&quot;blue&quot;) + annotate(&quot;text&quot;, label =&quot;Blue horizontal line = posterior probability Ho = 0.05&quot;, x=0, y=.1, hjust=0) + ggtitle(&quot;Posterior probability Ho is true&quot;, subtitle = &quot;Varying levels of Bayes factors from weak (0.2) to strong (0.01))&quot;) + theme_bw() This graph demonstrates that p = 0.05 (Bayes factor = 0.15) tend to lead to an overestimation of the effect. As calculated above, if the prior probability = 50%, then p = 0.05 leads to a posterior probability of approximately 13%. Alternatively, if one observes a p value = 0.05, and desires assurance that the posterior probability &lt; 5% then the prior probability of Ho being true must not be greater than 27%. Let us apply to these concepts to a real life research finding that was published in the NEJM concerning door to balloon time and mortality among patients undergoing primary percutaneous coronary intervention (PCI). Bayes factors - real life clinical example Past studies have repeatedly and consistently shown that reducing treatment times (“time is muscle”) improves outcomes for patients with ST elevation myocardial infarction with no previous notion of a threshold time where further improvements with decreasing time are no longer realizable. These NEJM authors analyzed trends in door-to-balloon times and in-hospital mortality using data from 96,738 admissions for patients undergoing primary PCI for ST-segment elevation myocardial infarction from July 2005 through June 2009 at 515 hospitals. The authors report median door-to-balloon times declined significantly, from 83 minutes in 2005 to 67 minutes in 2008 (P&lt;0.001) but no significant overall change in in risk-adjusted in-hospital mortality (5.0% in 2005–2006 and 4.7% in 2008–2009, P=0.34). The authors conclude “Although national door-to-balloon times have improved significantly for patients undergoing primary PCI for ST-segment elevation myocardial infarction, in-hospital mortality has remained virtually unchanged. These data suggest that additional strategies are needed to reduce in-hospital mortality in this population.” First let’s see if we can reproduce the original results and then re-analyze the data using Bayes Factors. As our purpose here is to demonstrate the evidential benefits of Bayes factors, we will be ignoring a large amount of prior information that decreasing time to treatment improves outcomes. # enter the NEJM raw data mat &lt;- matrix(c(938,1238,17822,25102), nrow=2, byrow = TRUE, dimnames = list(Outcome = c(&quot;Dead&quot;, &quot;Alive&quot;), Year = c(&quot;2005&quot;, &quot;2008&quot;))) mat ## Year ## Outcome 2005 2008 ## Dead 938 1238 ## Alive 17822 25102 result &lt;- prop.test(mat[1,],mat[2,]+mat[1,]) result ## ## 2-sample test for equality of proportions with continuity correction ## ## data: mat[1, ] out of mat[2, ] + mat[1, ] ## X-squared = 2.1, df = 1, p-value = 0.1 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.001079 0.007077 ## sample estimates: ## prop 1 prop 2 ## 0.050 0.047 paste(&quot;More precise p value = &quot;, round(result$p.value,2)) ## [1] &quot;More precise p value = 0.15&quot; This confirms the results with 5.0% versus 4.7% a difference that is indeed statistically insignificant (p &gt; 0.05). However while the conventional frequentist analysis can’t reject Ho of no mortality Δ, the data actually provides more support for the alternative hypothesis of a decrease in mortality in mortality and provides diminished support for Ho of no mortality Δ for the observed 16 minute reduction in time to treatment. Exactly how much support for these competing hypotheses can be determined using Bayes factors. # determine Z value from the observed p value p &lt;- 0.1491 Z &lt;- qnorm(1-(p/2)) min_bf(Z) ## [1] &quot;Minimum Bayes Factor = 0.35 so there is 2.83 times more evidence supporting the alternative hypothesis of the observed data than for the null of no benefit&quot; This analysis may also be interpreted as i) the observed results are 1/3 as probable under the null hypothesis of no difference in mortality as they are under the alternative of a 3 /1000 decrease ii) the evidence supports the null hypothesis only 1/3 as strongly as it does the alternative iii) the odds of the null hypothesis relative to the alternative hypothesis after the experiment are 1/3 what they were before the experiment In the text, the authors state “We have reached a threshold such that time to reperfusion no longer matter, provided &lt;90 minutes, and we need to look elsewhere for improvements.” This suggests the following conclusion may be more appropriate “This study shows that improved treatment times, even below the 90 minute threshold, are likely associated with meaningful mortality benefits that are entirely consistent with previous work, strengthen the evidence that quicker time to treatment improves outcomes and emphasizes the huge public health impact that continued quality improvement to decrease time to treatment holds. Efforts should continue to reduce all treatment delays as well as to search elsewhere for improvements.” As this article has been cited over 400 times, correctly interpreting this data is essential. References "],["design.html", "Chapter 5 Non-experimental designs 5.1 Introduction 5.2 Cohort studies 5.3 Case control 5.4 Cross sectional 5.5 Miscellaneous designs", " Chapter 5 Non-experimental designs 5.0.1 R packages required for this chapter library(knitr) library(epiR) 5.1 Introduction This chapter will provide only the briefest review of non-experimental study designs so as to try and assure that the reader has a common baseline knowledge of the advantages and limitations of each design. The excellent introductory (Gordis 2014) (Rothman 2012), intermediate (Szklo and Nieto 2019) and advanced (Rothman, Greenland, and Lash 2008) epidemiology textbooks referred to in Chapter 1.2 provide more complete information. The common hierarchy of evidence based medicine (EBM) research designs is presented in this ubiquituous pyramid schema. Figure 5.1: EBM pyramid of research designs However, Figure 5.1 is another example of a simple heuristic which enables quick but often erroneous conclusions concerning study design. Unfortunately as shown in this and susequent chapters, there are no shortcuts to the evaluation of study designs which need individual considerations. In that respect the following pyramid is both more realistic and helpful. Figure 2.1: EBM pyramid of research designs Although one may debate that the magnitude of the blue area is an overestimate of the scale of good designs, it does underline the importance in clinical epidemiology of assessing individual study quality for both experimental and non-experimental studies. As this book is orientated toward clinical epidemiology, experimental designs (randomized clinical trials (RCTs)) will be emphasized and discussed separately in a later chapter. RCTs have a special emphasis since they are often considered the pinnacle of research designs and greatly influence medical guidelines and consequently clinical practice. The following is a taxonomy of the different types of study designs. Figure 2.4: Overview of different research designs Ecological study designs involve populations or groups of individuals as the unit of analysis as opposed to the other observation designs where the unit of analysis is the individual. Ecological studies are especially useful for descriptive reports, the analysis of birth cohorts, when exposure is only available at the group level, or to investigate differences between populations when the between population difference is much greater than within population differences. For example, ecological studies would be appropriate for aggregate exposure involving air pollution, health care systems, or gun control laws. One must be careful to avoid the ecological bias that can occur because an association observed between variables on the aggregate level does not necessarily represent the association that exists at the individual level. The ecological bias can be shown graphically in Figure 5.2 where the discordance of the exposure association between groups and individuals is especially strong. Figure 5.2: Ecological bias The real problem is cross-level reference. For an ecological study the level of measurement, level of analysis, and level of inference must all remain at the group level. Ecological studies will not be considered further in this book. Among non-experimental designs involving individuals, there are essentially 3 different ways at arriving at conclusions by 1) reference to population follow-up (cohort) 2) joint assessment of exposure among cases and non-cases (case-control) 3) reference to one particular time (cross-sectional) Since all study designs, including the non-experimental (cohort, case-control, cross-sectional) either secretly or not, aim to estimate similar causal quantities, it could be argued that emphasizing their distinctions is somewhat artificial. In other words, do not mistake the journey (particular study type) for the destination (causal effect of an exposure). This lack of randomization with observational studies, is why we are continually reminded that non-experimental studies can’t provide evidence for causality. Instead we are told to talk of associations. A list of the top 2019 JAMA Internal Medicine articles as determined by Altmetric scores can be found here and is reproduced in Figure 5.3. Figure 5.3: JAMA Internal Medicine Top 2019 Altmetric Articles Most interestingly, 11 of the top 14 articles have “association” in their title. But are we really interested in mere associations of the type that matches in your pocket or yellow fingers are associated with lung cancer? I don’t believe so and suspect most people are more interested for obvious reasons in causality and whether they care to admit it or not are subconsciously interpreting association studies in this light(Hernan 2018) . Otherwise there appears little justification to waste one’s time on reading these articles. Although this is not a book on causality and although I most definitely lack the expertise to delve deeply into this issue, I do think we should acknowledge that the heuristic that observational studies can never inform on causality is flawed and should think more deeply about how we can reach this elusive goal of establishing causality. Please see the following excellent references on causal inference, (Westreich 2019), (Pearl, Glymour, and Jewell 2016), (Hernán and JM 2020) and (Pearl 2008), ordered in increasing detail and complexity. Key concepts for causal inference Temporality (exposure must occur before the outcome) Conditional exchangeability (to minimize confounding and selection bias to achieve comparability using regression, stratification, restriction, or standardization) Positivity and consistency (to assure the existence of a potential counterfactual) No measurement error (to minimize misclassification in exposure, outcome or confounders required to achieve conditional exchangeability) Again more details may be found in the above mentioned references. 5.2 Cohort studies A cohort is a designated/defined group of individuals followed through time often to study incidence of disease in the study group. Examples of sample cohorts may include occupational cohorts, specific groups at risk of a disease or convenience samples (e.g. Nurses or Framingham cohorts). Be careful to distinguish between study, source and target populations. Cohort studies offer multiple potential advantages: * Can study exposures that are difficult or unthinkable to randomize * Study population often more representative of target pop’l * Allows calculation of incidence rates * Time sequence is generally clear (exposure before outcome) * Efficient as multiple outcomes / exposures can be assessed as new hypothesis are developed over time Cohort studies can be of 2 formats; 1. Concurrent (prospective) cohort studies: assembled at present time. Advantages: measurement of exposure, outcome, covariates is decided at baseline and can see temporal ordering of exposure and disease. Disadvantages: expensive and time consuming. 2. Historical/non-concurrent/mixed (retrospective) cohort studies: incorporates historical time exposed (at least partially). Advantages: less expensive; linking data registries (e.g. exposure and outcome information). Disadvantages: can only use available information; possibly lower quality measurements; data may not have been collected for research purposes. In cohort studies, exposed/unexposed groups exist in the source population and are selected by the investigator while in an RCT, a form of closed cohort, treatment/exposure is assigned by the investigator. Figure 5.4: Observational research design Figure 5.5: Experimental research design Practically, the best general approach to achieve valid causal non-experimental designs is to try to emulate the RCT(Hernan and Robins 2016) you would like to do with special attention to the following: * Selection of population * Exposure definition (induction, duration, intensity, cumulative exposures) * Outcome ascertainment with minimization of lost to follow-up 5.3 Case control Sometimes it useful to start with the cases! Although important for all research designs, it is obviously essential for case control designs to have an unambiguous, valid case definition preferably using objective and established criteria that avoids any misclassification or other biases. Careful distinction between incident and prevalent cases is also of prime importance. Where the cases are found is a function of the particular research question and setting. Potential sources include hospital registers, vital records (births/deaths), national registries (e.g., for cancer, diabetes) and community clinics. After case identification, the most important and difficult next step is the selection of the controls. Consideration of a counterfactual model can help operationalize the choice of controls. Controls are drawn from a sample of individuals without the disease, selected from the same reference population that originated the cases and who go through the same selection processes as the cases. Moreover, controls must be individuals who, if they had developed the case disease, would have been included in the case group. Case control studies may be conducted with an open or closed study population. In a dynamic (open) population, there are two options for selecting controls; i) if the population is in a steady-state, sample randomly from the person-time distribution ii) if not, controls may be selected at the same time as cases occur (i.e., “matched on time”). In a closed study population, there are three options for selecting controls; i) at the end of follow-up ii) at the beginning of follow-up iii) throughout follow-up as cases occur (“matched on time”). Analytically, these distinctions lead to different effect measures, each of which (under various assumptions) parallels an equivalent measure from a full-cohort study. Figure 2.6: Case control sampling times Sample point 1 - This is the classic “case based” sampling (AKA “exclusive,” “cumulative”) that occurs at the end of follow-up. In this case, the incident odds ratio (OR) ≈ risk ratio (RR) (under the rare disease assumption) Sample point 2 - This is “case-cohort” sampling (AKA “inclusive”) that occurs at the beginning of follow-up. In this case, the incident odds ratio (OR) ≈ risk ratio (RR) (rare disease assumption not required) Sample point 3 - “Nested” sampling (AKA “incidence density”) from the distribution of exposed person-time matched during follow-up. In this case, the incident odds ratio (OR) ≈ rate ratio (RR) The efficiency of the case control design comes from taking a sample, and not all, of the controls. Under that logic, it may be reasonably asked why not take only a sample of the cases? Consider the following example. set.seed(1234) dat &lt;- matrix(c(200,100,1800,1900), nrow=2, dimnames = list(c(&quot;Not exposed&quot;,&quot;Exposed&quot;), c(&quot;MI yes&quot;, &quot;MI no&quot;))) kable(dat, caption = &quot;Full cohort&quot;) Table 5.1: Full cohort MI yes MI no Not exposed 200 1800 Exposed 100 1900 full &lt;- epi.2by2(dat) dat.con10 &lt;- dat dat.con10[,2] &lt;- dat[,2]/10 kable(dat.con10, caption = &quot;Case control - 10% of controls&quot;) Table 5.1: Case control - 10% of controls MI yes MI no Not exposed 200 180 Exposed 100 190 full.10 &lt;- epi.2by2(as.matrix(dat.con10)) dat.case10 &lt;- dat dat.case10[,1] &lt;- dat[,1]/10 dat.case10[,2] &lt;- dat[,2]/10 kable(dat.case10, caption = &quot;Case control - 10% of cases and controls&quot;) Table 5.1: Case control - 10% of cases and controls MI yes MI no Not exposed 20 180 Exposed 10 190 full.case10 &lt;- epi.2by2(as.matrix(dat.case10)) The OR is 2.11 with 95% CI 1.65 - 2.71. The OR is fairly close to incident risk ratio for the full cohort (RR = 2) since the rare assumption is approximately true, about 10% incidence. If we select only 1/10 of the controls the OR is 2.11 with 95% CI 1.54 - 2.89, a trivial difference. On the other hand, suppose we take a 1/10 sample of the cases, the OR remains unbiased at 2.11 but the 95% CI 0.96 - 4.63 is much larger. It is this lack of precision that mandates the inclusion of all cases in a case / control design. This is easily understood when it is recalled that the standard error of the estimated OR is \\[se(\\hat{OR}) = \\sqrt{\\dfrac{1}{a} + \\dfrac{1}{b} + \\dfrac{1}{c} + \\dfrac{1}{d}}\\] so the largest component comes from the smallest cell entries and se will be minimized by taking all the cases. The following figure from (Knol et al. 2008) is a useful summary of the effect measures available from case control studies depending on the nature of the cases (prevalent or incident; level 1), the type of source population (fixed cohort or dynamic population; level 2), the sampling design used to select controls (level 3), and the underlying assumptions (level 4). Figure 1.2: Effect measures from case control designs In summary, case control studies have the advantages of being faster to perform and less expensive to conduct than cohort studies but care must be exercised that they, like all study designs, are carefully performed. Proper control selection is essential and must come from the same target population as cases (easiest when performed within an established cohort). Controls must be sampled independently of exposure and there is improved precision with more controls (1,2,3,4) but diminishing returns (SE 0.167, 0.145, 0.138, 0.134). Effect measure precision is improved precision by taking all the cases. Although case control studies are susceptible to recognized biases (Berkson, recall, incidence/prevalence) these can be avoided with necessary care. The routine placement of case-control studies under cohort studies on hierarchies of study designs is not well-founded. An interesting variant is the case crossover design where each case serves as its own control thereby minimizes confounding for time invariant factors whether observed or unobserved. Exposures must vary over time but have a short induction time, a transient effect (e.g., triggers for heart attacks, asthma episodes) and no cumulative effects. This design is the observational analogue of the crossover randomized trial. A final variant are case series without any controls. While the early identification of cases may prove sentinel for a new disease (see cases series that lead to first identification of the AIDS epidemic), the inferential strength of this design is limited due to the lack of any suitable comparator. Moreover arbitrary selection of cases and an embellished narrative can lead to an undervaluing of scientific evidence and great public health danger (see cases series (Wakefield et al. 1998), later retracted, at the genesis of the vaccine autism falsehood). 5.4 Cross sectional Cross sectional studies are most useful for descriptive epidemiology with a primary goal of estimating disease prevalence. As no follow-up is required, cross sectional studies are fast, efficient and can enroll large numbers of participants. However they have little value for causal inference as they provide no information on timing of outcome relative to exposure (temporality) and include only those individuals alive at the time of the study, thereby introducing a prevalence-incidence bias. Due to these limitations, this study design has little value in clinical epidemiology and will not be discussed further. 5.5 Miscellaneous designs There are, of course, many variants and other miscellaneous non-experimental designs including difference-in-difference (DID), regression discontinuity, and quasi-experimental to name but a few. Conceptually, DID design can be best thought of as a combination of a before &amp; after comparison and a comparison between treated &amp; untreated individuals and are therefore also known as ontrolled before and after studies. These studies minimize bias due to pretreatment differences in outcomes,allow for a flexible control of time invariant confounders and are preferable to an uncontrolled before and after comparison of only treated individuals. Quasi-experimental designs refer to approaches to effect estimation in which investigators identify (or create) a source of variation in the exposure which is unrelated to the rest of the causal system under study—including the outcome (except through the exposure itself) and the confounders. A classic historical example is John Snow’s cholera work where which household received “dirtier” water from the Southwark and Vauxhall company or “cleaner” water from the Lambeth company was a quasi-random event. The company can thus be seen as an instrumental variable, similar to randomization. Regression discontinuity designs are a special subset of quasi-experimental designs where subjects just above and below a given threshold are essentially identical on all observed and unobserved characteristics yet are arbitrarily assigned different therapies. References "],["references.html", "Chapter 6 References", " Chapter 6 References sessionInfo() ## R version 4.0.3 (2020-10-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] bookdown_0.21.6 digest_0.6.27 R6_2.5.0 jsonlite_1.7.2 ## [5] magrittr_2.0.1 evaluate_0.14 stringi_1.5.3 rlang_0.4.10 ## [9] jquerylib_0.1.3 bslib_0.2.4 rmarkdown_2.6.6 tools_4.0.3 ## [13] stringr_1.4.0 xfun_0.20 yaml_2.2.1 parallel_4.0.3 ## [17] compiler_4.0.3 htmltools_0.5.1.1 knitr_1.31 sass_0.3.1 "]]
